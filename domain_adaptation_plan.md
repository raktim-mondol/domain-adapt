# Comprehensive Domain Adaptation Plan for Mine Site Spoil Classification

## Executive Summary

This document outlines a comprehensive strategy to adapt the current BMA MIL (Multiple Instance Learning) spoil classification model to generalize across different mine sites. The model currently achieves 83-88% accuracy on the source mine site but shows degraded performance on new mine sites due to domain shift.

**Key Challenge**: Cross-mine site domain adaptation where visual characteristics (lighting, soil composition, camera angles, weathering patterns) vary significantly between locations.

**Solution Approach**: Multi-stage domain adaptation pipeline combining state-of-the-art techniques specifically designed for vision-based classification with limited target domain labels.

---

## 0. FINALIZED METHOD - READY FOR REVIEW

---

## ğŸ“‹ Executive Decision

**Selected Method**: **Hybrid DANN + MMD + Orthogonal Constraint (Supervised)**

**Why This Method**:
- âœ… **DANN**: Proven adversarial domain adaptation (stable with GRL)
- âœ… **MMD**: Statistical distribution matching (no adversarial instability)
- âœ… **Orthogonal Constraint**: Critical safeguard against over-alignment
- âœ… **Modular Design**: Easy to add contrastive loss later
- âœ… **Supervised**: Both domains labeled â†’ stronger training signal

---

## 0.1 Problem Setup - Your Specific Scenario

### Data Configuration

| Domain | Name | Role | Labels | Classes | Status |
|--------|------|------|--------|---------|--------|
| Source | **QLD1** | Training Site | âœ… Available | 3 (Cat1, Cat2, Cat3) | Fully labeled |
| Target | **QLD2** | New Site | âœ… Available | 3 (Cat1, Cat2, Cat3) | Fully labeled |

### Scenario Type: **Supervised Domain Adaptation**

**Key Advantage**: You have labels for BOTH QLD1 and QLD2!
- Can use QLD2 labels directly during training (no pseudo-labeling needed)
- Stronger supervision signal â†’ better performance
- Faster convergence (typically 20-30% fewer epochs)
- Expected 5-10% accuracy boost compared to unsupervised methods

**Training Strategy**:
```
Source Batch (QLD1) + Target Batch (QLD2)
        â†“                        â†“
   [Labels: Cat1-3]        [Labels: Cat1-3]
        â†“                        â†“
    L_cls_source  +  L_cls_target  (both contribute to classification loss!)
        â†“
    + L_adv (domain confusion via DANN)
    + L_mmd (statistical alignment)
    + L_orth (prevent over-alignment)
```

---

## 0.2 Finalized Architecture

### Overall System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT: QLD1 or QLD2 Images                    â”‚
â”‚                    (Patches from pile images)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EXISTING: Feature Extractor (ViT-R50)               â”‚
â”‚              Pre-trained on ImageNet, fine-tunable               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           EXISTING: MIL Attention Aggregator (512-dim)           â”‚
â”‚           Aggregates patch features to bag features              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
          Bag Features (512-dim)
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Class Classifierâ”‚   â”‚ Domain Discriminator   â”‚
â”‚   (3 classes)   â”‚   â”‚  (Source vs Target)    â”‚
â”‚                 â”‚   â”‚  + Gradient Reversal   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                        â†“
    Class Logits            Domain Logits
         â†“                        â†“
    L_cls (CE)              L_adv (BCE + GRL)
```

### Loss Functions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       LOSS COMPUTATION                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Classification Loss (Supervised - both domains!)
   L_cls = CE(source_logits, source_labels) + CE(target_logits, target_labels)

2. Adversarial Loss (DANN with Gradient Reversal)
   L_adv = BCE(source_domain_logits, 0) + BCE(target_domain_logits, 1)
   Note: Gradient reversed during backprop

3. MMD Loss (Statistical Distribution Matching)
   L_mmd = MMD(source_features, target_features)
   Multi-kernel with Gaussian RBF

4. Orthogonal Constraint (Critical!)
   L_orth = ||W_classifier^T Ã— W_discriminator||Â²
   Prevents over-alignment and class collapse

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TOTAL LOSS = L_cls + Î»_advÂ·L_adv + Î»_mmdÂ·L_mmd + Î»_orthÂ·L_orthâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 0.3 Modular Design for Future Extensions

### Component Architecture

```python
# Modular design - easy to extend

class DomainAdaptationLossManager:
    """Plugin-based loss management"""

    def __init__(self):
        self.losses = {}  # Dict of loss functions
        self.weights = {}  # Loss weighting

    def register_loss(self, name, loss_fn, weight):
        """Register a new loss component"""
        self.losses[name] = loss_fn
        self.weights[name] = weight

    def compute_total_loss(self, model_outputs):
        """Compute weighted sum of all losses"""
        total = 0
        loss_dict = {}

        for name, loss_fn in self.losses.items():
            loss_value = loss_fn(model_outputs)
            loss_dict[name] = loss_value
            total += self.weights[name] * loss_value

        return total, loss_dict

# Current Phase 1 Implementation:
loss_manager = DomainAdaptationLossManager()
loss_manager.register_loss('classification', classification_loss, weight=1.0)
loss_manager.register_loss('adversarial', adversarial_loss, weight=0.5)
loss_manager.register_loss('mmd', mmd_loss, weight=0.3)
loss_manager.register_loss('orthogonal', orthogonal_loss, weight=0.1)

# Future Phase 2 - Simply add:
# loss_manager.register_loss('contrastive', contrastive_loss, weight=0.2)
# No need to modify training loop!
```

### File Structure

```
classification_model/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bma_mil_model.py              [EXISTING - Keep as is]
â”‚   â”‚   â””â”€â”€ domain_adaptive_mil.py        [NEW - Main DA model]
â”‚   â”‚
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”œâ”€â”€ __init__.py                   [NEW]
â”‚   â”‚   â”œâ”€â”€ loss_manager.py               [NEW - Modular loss system]
â”‚   â”‚   â”œâ”€â”€ adversarial_loss.py           [NEW - DANN with GRL]
â”‚   â”‚   â”œâ”€â”€ mmd_loss.py                   [NEW - Multi-kernel MMD]
â”‚   â”‚   â”œâ”€â”€ orthogonal_loss.py            [NEW - Orthogonality constraint]
â”‚   â”‚   â””â”€â”€ contrastive_loss.py           [FUTURE - Phase 2]
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py                    [EXISTING - Keep]
â”‚   â”‚   â””â”€â”€ domain_dataset.py             [NEW - QLD1/QLD2 loader]
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ training.py                   [EXISTING - Keep]
â”‚   â”‚   â””â”€â”€ domain_adaptive_training.py   [NEW - DA training loop]
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ gradient_reversal.py          [NEW - GRL layer]
â”‚       â””â”€â”€ domain_utils.py               [NEW - Helper functions]
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config.py                         [EXISTING - Keep]
â”‚   â””â”€â”€ domain_adaptation_config.py       [NEW - DA hyperparameters]
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ train.py                          [EXISTING - Keep]
    â””â”€â”€ train_domain_adaptation.py        [NEW - Main DA training script]
```

---

## 0.4 Detailed Component Specifications

### Component 1: Gradient Reversal Layer (GRL)

**Purpose**: Enable adversarial training without separate discriminator optimizer

**Implementation**:
- Forward pass: Identity (pass features through unchanged)
- Backward pass: Reverse gradients with scaling factor Î±
- Î± schedule: 0 â†’ 1 over training (gradual adaptation)

**File**: `src/utils/gradient_reversal.py`

---

### Component 2: MMD Loss

**Purpose**: Minimize statistical distance between source and target feature distributions

**Configuration**:
- Multi-kernel: 5 Gaussian kernels with different bandwidths
- Kernel multiplier: 2.0 (bandwidth_k = 2^k)
- Applied to bag-level features (after MIL aggregation)

**File**: `src/losses/mmd_loss.py`

---

### Component 3: Orthogonal Constraint

**Purpose**: **CRITICAL** - Prevent over-alignment and class collapse

**Implementation**:
- Compute: ||W_classifier^T Ã— W_discriminator||Â²
- Use first layer weights (common feature space)
- Normalize weights before dot product
- Target: Minimize orthogonality metric

**File**: `src/losses/orthogonal_loss.py`

**Why Critical**: Without this, DANN + MMD will over-align and destroy class boundaries!

---

### Component 4: Domain Adaptive MIL Model

**Integration with Existing Code**:
- **Reuse**: Existing ViT-R50 feature extractor
- **Reuse**: Existing AttentionAggregator
- **Reuse**: Existing class classifier structure
- **Add**: Domain discriminator (new small network)
- **Add**: Loss computation methods

**File**: `src/models/domain_adaptive_mil.py`

---

### Component 5: Domain Dataset Loader

**Purpose**: Load QLD1 and QLD2 with domain labels

**CSV Format**:
```csv
pile,image_path,BMA_label,domain
pile_001,/path/to/img1.jpg,0,QLD1
pile_002,/path/to/img2.jpg,1,QLD1
pile_100,/path/to/img100.jpg,2,QLD2
...
```

**Features**:
- Domain-stratified splitting (keep source/target separate)
- Balanced sampling (equal batches from QLD1 and QLD2)
- Compatible with existing BMADataset structure

**File**: `src/data/domain_dataset.py`

---

## 0.5 Hyperparameter Configuration

### Loss Weights

| Loss Component | Symbol | Initial Value | Range | Schedule |
|----------------|--------|---------------|-------|----------|
| Classification | - | 1.0 | Fixed | None |
| Adversarial | Î»_adv | 0.5 | 0.3 - 1.0 | Constant or increase |
| MMD | Î»_mmd | 0.3 | 0.1 - 0.5 | Constant |
| Orthogonal | Î»_orth | 0.1 | 0.05 - 0.2 | Constant |

### GRL Alpha Schedule

```
Î±(epoch) = 2 / (1 + exp(-10 * epoch / max_epochs)) - 1

Epoch 0:   Î± â‰ˆ 0.0  (no domain confusion)
Epoch 25:  Î± â‰ˆ 0.9  (strong domain confusion)
Epoch 50+: Î± â‰ˆ 1.0  (maximum domain confusion)
```

### Training Configuration

```python
# Domain Adaptation Training Config
DOMAIN_ADAPTATION = {
    # Data
    'source_domain': 'QLD1',
    'target_domain': 'QLD2',
    'batch_size': 6,  # Same as existing
    'domain_balanced_sampling': True,  # Equal source/target per batch

    # Model
    'use_pretrained_weights': 'models/best_bma_mil_model.pth',  # Optional
    'freeze_feature_extractor': False,
    'trainable_feature_layers': 2,  # Fine-tune last 2 ViT blocks

    # Loss weights
    'lambda_adv': 0.5,
    'lambda_mmd': 0.3,
    'lambda_orth': 0.1,

    # Training
    'num_epochs': 100,
    'learning_rate': 1e-4,
    'optimizer': 'adamw',
    'weight_decay': 1e-5,

    # Monitoring (detect over-alignment)
    'monitor_source_accuracy': True,
    'early_stop_on_source_drop': True,
    'source_acc_threshold': 0.95,  # Stop if drops below 95% of baseline

    # MMD
    'mmd_kernel_mul': 2.0,
    'mmd_kernel_num': 5,

    # GRL schedule
    'grl_alpha_schedule': 'exponential',
    'grl_max_alpha': 1.0,
}
```

---

## 0.6 Training Protocol

### Phase 1: Baseline Evaluation (Week 1)

**Step 1.1**: Train baseline on QLD1 only
- Standard MIL training (existing pipeline)
- Save best checkpoint
- Evaluate on QLD1 test set â†’ Baseline accuracy

**Step 1.2**: Evaluate zero-shot transfer to QLD2
- Load QLD1-trained model
- Evaluate on QLD2 test set â†’ Zero-shot accuracy
- Compute domain gap = QLD1_acc - QLD2_acc

**Expected Results**:
- QLD1 accuracy: 83-88%
- QLD2 accuracy: 50-60%
- Domain gap: 25-35%

---

### Phase 2: Domain Adaptation Training (Weeks 2-3)

**Training Loop**:
```
For each epoch:
    For each iteration:
        # Sample balanced batch
        source_batch, source_labels = sample_from_QLD1()
        target_batch, target_labels = sample_from_QLD2()

        # Forward pass
        source_outputs = model(source_batch, domain='source')
        target_outputs = model(target_batch, domain='target')

        # Compute losses
        L_cls = CE(source_outputs, source_labels) + CE(target_outputs, target_labels)
        L_adv = adversarial_loss(source_features, target_features)
        L_mmd = mmd_loss(source_features, target_features)
        L_orth = orthogonal_constraint(model)

        # Total loss
        L_total = L_cls + Î»_adv*L_adv + Î»_mmd*L_mmd + Î»_orth*L_orth

        # Backprop and update
        optimizer.zero_grad()
        L_total.backward()
        optimizer.step()

    # Validation
    qld1_val_acc = evaluate(model, qld1_val_set)
    qld2_val_acc = evaluate(model, qld2_val_set)

    # Over-alignment check
    if qld1_val_acc < baseline_qld1_acc * 0.95:
        print("WARNING: Source accuracy dropped - possible over-alignment!")
        # Increase Î»_orth or stop training
```

---

### Phase 3: Evaluation (Week 3)

**Metrics to Report**:

1. **Accuracy**:
   - QLD1 test accuracy (should maintain ~80-86%)
   - QLD2 test accuracy (target: 75-82%)
   - Domain gap (target: <10%)

2. **Per-Class Performance**:
   - F1-score for Cat1, Cat2, Cat3 on both domains
   - Confusion matrices

3. **Domain Alignment**:
   - MMD distance (should decrease during training)
   - Domain classification accuracy (should approach 50%)

4. **Over-Alignment Monitoring**:
   - Source accuracy trend (should not drop)
   - Orthogonality loss trend
   - Feature variance (should not collapse)

---

## 0.7 Expected Outcomes

### Performance Targets

| Metric | Before DA | After DA | Improvement |
|--------|-----------|----------|-------------|
| QLD1 Accuracy | 85% | 83-86% | Maintained |
| QLD2 Accuracy | 55% | 75-82% | **+20-27%** |
| Domain Gap | 30% | <10% | **-20%** |
| Cat1 F1 (QLD2) | 0.40 | 0.70-0.80 | +0.30-0.40 |
| Cat2 F1 (QLD2) | 0.50 | 0.72-0.82 | +0.22-0.32 |
| Cat3 F1 (QLD2) | 0.60 | 0.75-0.85 | +0.15-0.25 |

### Success Criteria

âœ… **Primary**:
- QLD2 accuracy > 75%
- QLD1 accuracy > 80% (maintained)
- Domain gap < 10%

âœ… **Secondary**:
- All classes F1 > 0.65 on QLD2
- Training converges in <100 epochs
- No over-alignment detected

---

## 0.8 Future Extensions (Phase 2)

### Adding Contrastive Loss (Future)

**When to Add**: If domain gap still >8% after DANN+MMD+Orth

**How to Add** (modular!):
```python
# Simply register new loss - no other changes needed!
from src.losses.contrastive_loss import ContrastiveDomainLoss

contrastive_loss = ContrastiveDomainLoss(temperature=0.07)
loss_manager.register_loss('contrastive', contrastive_loss, weight=0.2)

# Update config
DOMAIN_ADAPTATION['lambda_contrastive'] = 0.2

# Done! Training loop automatically uses it.
```

**Expected Benefit**: Additional 2-3% accuracy on QLD2

---

## 0.9 Implementation Checklist

### Week 1: Foundation
- [ ] Create `src/utils/gradient_reversal.py` (GRL layer)
- [ ] Create `src/losses/mmd_loss.py` (multi-kernel MMD)
- [ ] Create `src/losses/orthogonal_loss.py` (orthogonality constraint)
- [ ] Create `src/losses/adversarial_loss.py` (DANN loss)
- [ ] Create `src/losses/loss_manager.py` (modular loss system)

### Week 2: Model & Data
- [ ] Create `src/models/domain_adaptive_mil.py` (main DA model)
- [ ] Create `src/data/domain_dataset.py` (QLD1/QLD2 loader)
- [ ] Create `configs/domain_adaptation_config.py` (DA config)
- [ ] Test model forward pass and loss computation

### Week 3: Training & Evaluation
- [ ] Create `src/training/domain_adaptive_training.py` (DA training loop)
- [ ] Create `scripts/train_domain_adaptation.py` (main script)
- [ ] Implement over-alignment monitoring
- [ ] Create evaluation script for cross-domain metrics

### Week 4: Experiments
- [ ] Baseline: Train on QLD1, test on QLD1 and QLD2
- [ ] Experiment 1: DANN only
- [ ] Experiment 2: MMD only
- [ ] Experiment 3: DANN + MMD
- [ ] Experiment 4: DANN + MMD + Orthogonal (final)
- [ ] Hyperparameter tuning (Î»_adv, Î»_mmd, Î»_orth)

---

## 0.10 Key Design Decisions - Summary

| Decision | Choice | Rationale |
|----------|--------|-----------|
| **Method** | DANN + MMD + Orthogonal | Stable, proven, complementary |
| **Scenario** | Supervised DA | Both domains labeled (use target labels!) |
| **Modularity** | Plugin-based loss manager | Easy to add contrastive loss later |
| **Over-alignment** | Orthogonal constraint | Critical safeguard for hybrid approach |
| **Integration** | Extend existing MIL model | Reuse ViT-R50 + attention aggregator |
| **Data** | QLD1 (source), QLD2 (target) | 3 classes each, fully labeled |
| **Training** | Joint optimization | Both domains in each batch |
| **Monitoring** | Source accuracy tracking | Early detection of over-alignment |

---

## âœ… READY FOR REVIEW

**This is the finalized plan. Please review and approve before implementation begins.**

**Key Questions for Review**:
1. âœ“ Approve DANN + MMD + Orthogonal constraint approach?
2. âœ“ Confirm QLD1 (source), QLD2 (target), both with 3 classes?
3. âœ“ Confirm both datasets have labels (supervised DA)?
4. âœ“ Approve modular design for future contrastive loss?
5. âœ“ Any additional requirements or constraints?

**Next Steps After Approval**:
- Implement components in order (GRL â†’ Losses â†’ Model â†’ Training)
- Test each component independently
- Integrate and run baseline evaluation
- Begin domain adaptation experiments

---

## 1. Problem Analysis

### 1.1 Domain Shift Characteristics in Mine Sites

**Source Domain** (Training Site):
- Specific geological composition and color profiles
- Consistent lighting conditions and camera setup
- Particular weathering and oxidation patterns
- Site-specific equipment and capture protocols

**Target Domains** (New Mine Sites):
- Different mineral compositions â†’ color shifts
- Varying illumination (time of day, weather, seasons)
- Different camera models and calibration
- Unique geological stratification patterns
- Site-specific environmental conditions

### 1.2 Types of Domain Shift

1. **Covariate Shift**: P_source(X) â‰  P_target(X)
   - Input distribution changes (color, texture, lighting)
   - Label distribution remains similar

2. **Label Shift**: P_source(Y) â‰  P_target(Y)
   - Different proportions of spoil classes
   - Some classes may be rare/absent at certain sites

3. **Concept Shift**: P_source(Y|X) â‰  P_target(Y|X)
   - Same visual appearance may indicate different classes
   - Geological context affects interpretation

### 1.3 Current System Limitations

| Component | Current Approach | Limitation |
|-----------|------------------|------------|
| Feature Extraction | ImageNet pre-trained ViT-R50 | Not adapted to mining domain |
| Training | Single-source supervised | No cross-domain alignment |
| Augmentation | Generic geometric transforms | Doesn't model domain shifts |
| Evaluation | Single-domain validation | No generalization metrics |
| Architecture | Standard MIL classifier | No domain-invariant learning |

---

## 2. State-of-the-Art Domain Adaptation Techniques

### 2.1 Deep Domain Adaptation Methods

#### 2.1.1 **Adversarial Domain Adaptation (ADA)**

**Concept**: Learn domain-invariant features by fooling a domain discriminator.

**Key Papers**:
- DANN (Domain-Adversarial Neural Networks) - Ganin et al., 2016
- ADDA (Adversarial Discriminative Domain Adaptation) - Tzeng et al., 2017
- CDAN (Conditional Domain Adversarial Network) - Long et al., 2018

**Architecture**:
```
Input Image â†’ Feature Extractor (ViT-R50) â†’ Features
                                                â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â†“                                           â†“
              Class Classifier                          Domain Discriminator
                  (3 classes)                          (Source vs Target)
                        â†“                                           â†“
              Classification Loss                 Adversarial Loss (GRL)
```

**Advantages**:
- Strong theoretical foundation
- Learns domain-invariant representations
- Proven effectiveness on visual tasks

**Considerations for MIL**:
- Apply adversarial loss at both patch and bag levels
- Use Gradient Reversal Layer (GRL) for stable training

---

#### 2.1.2 **Maximum Mean Discrepancy (MMD)**

**Concept**: Minimize statistical distance between source and target feature distributions.

**Key Papers**:
- Deep Domain Confusion (DDC) - Tzeng et al., 2014
- Deep Adaptation Networks (DAN) - Long et al., 2015
- Joint Adaptation Networks (JAN) - Long et al., 2017

**Loss Function**:
```
MMD(Xs, Xt) = ||1/ns Î£Ï†(xs_i) - 1/nt Î£Ï†(xt_j)||Â²
```

**Multi-Kernel MMD**:
```
MMD_k = Î£_k Î²_k * MMD(Xs, Xt; kernel_k)
```

**Advantages**:
- No additional discriminator needed
- Explicit distribution matching
- Effective for covariate shift

**Implementation Strategy**:
- Apply MMD at multiple feature levels (patch, bag, pile)
- Use multiple kernels (Gaussian with different bandwidths)

---

#### 2.1.3 **Self-Training and Pseudo-Labeling**

**Concept**: Use confident predictions on target domain as pseudo-labels for retraining.

**Key Papers**:
- Noisy Student - Xie et al., 2020
- Meta Pseudo Labels - Pham et al., 2021
- FixMatch - Sohn et al., 2020

**Pipeline**:
```
1. Train on source domain
2. Generate pseudo-labels for target domain (high confidence only)
3. Retrain with source labels + target pseudo-labels
4. Iterate with confidence threshold scheduling
```

**Advanced Variant - Teacher-Student Framework**:
- Teacher model: Generates pseudo-labels
- Student model: Trained on pseudo-labels + strong augmentation
- Progressive self-training with EMA updates

**Advantages**:
- No target domain labels required
- Simple to implement
- Effective when model confidence is reliable

---

#### 2.1.4 **Contrastive Domain Adaptation**

**Concept**: Learn representations where same-class samples cluster together across domains.

**Key Papers**:
- SupCon (Supervised Contrastive Learning) - Khosla et al., 2020
- CrossMatch - Saito et al., 2021
- CD3A (Contrastive Domain Discrepancy for Domain Adaptation) - Zhao et al., 2021

**Loss Function**:
```
L_contrastive = -log(exp(sim(z_i, z_i+)/Ï„) / Î£_j exp(sim(z_i, z_j)/Ï„))

Where:
- z_i: anchor sample features
- z_i+: positive samples (same class, any domain)
- z_j: all samples in batch
- Ï„: temperature parameter
```

**Domain-Aware Contrastive Loss**:
```
L_DAC = L_intra-class + L_inter-domain + L_inter-class
```

**Advantages**:
- Learns discriminative features
- Explicitly reduces domain gap
- Effective with limited target labels

---

#### 2.1.5 **Meta-Learning for Domain Adaptation**

**Concept**: Learn to quickly adapt to new domains with few samples.

**Key Papers**:
- MAML (Model-Agnostic Meta-Learning) - Finn et al., 2017
- Meta-DAN - Ma et al., 2019
- DLOW (Domain-specific Learning with Optimization-based Weights) - Gao et al., 2020

**Training Strategy**:
```
Meta-Training Phase:
- Simulate domain shift using data augmentation
- Learn initialization that adapts quickly
- Optimize for few-shot adaptation

Meta-Testing Phase:
- Fine-tune on few labeled target samples
- Fast adaptation to new mine sites
```

**Advantages**:
- Few-shot learning capability
- Generalizes to unseen domains
- Suitable for limited target data scenarios

---

### 2.2 Domain-Specific Augmentation Techniques

#### 2.2.1 **Style Transfer for Domain Bridging**

**Concept**: Transform source images to mimic target domain appearance.

**Key Techniques**:
- **AdaIN (Adaptive Instance Normalization)** - Huang et al., 2017
- **CycleGAN** - Zhu et al., 2017
- **Neural Style Transfer**

**Application to Mine Sites**:
```
Source Site Images â†’ Style Transfer â†’ "Target-style" Images
                                              â†“
                                     Train with both original
                                     and style-transferred images
```

**Mining-Specific Augmentations**:
- Color shift simulation (mineral composition variations)
- Illumination changes (time-of-day, weather)
- Texture synthesis (weathering patterns)
- Camera response simulation

---

#### 2.2.2 **Adversarial Data Augmentation**

**Concept**: Generate augmentations that maximize domain confusion.

**Implementation**:
```python
# Learn domain-specific augmentation parameters
aug_params = AugmentationGenerator(domain_classifier_loss)
augmented_images = apply_augmentation(images, aug_params)
```

**Benefits**:
- Data-driven augmentation strategy
- Specifically targets domain gap
- No need for target domain data during augmentation design

---

### 2.3 Test-Time Adaptation Techniques

#### 2.3.1 **Test-Time Training (TTT)**

**Concept**: Continue adapting model during inference on target domain.

**Key Papers**:
- TTT - Sun et al., 2020
- TENT (Test Entropy Minimization) - Wang et al., 2021
- NOTE (Normalization for Test-Time Adaptation) - Gong et al., 2022

**Strategy**:
```
At inference time on new mine site:
1. Collect unlabeled test samples
2. Adapt batch normalization statistics
3. Minimize entropy of predictions
4. Update model parameters
```

**Advantages**:
- No target labels needed
- Adapts to test distribution
- Can be combined with other methods

---

#### 2.3.2 **Online Domain Adaptation**

**Concept**: Incrementally adapt as new samples arrive.

**Implementation**:
```
Initial model (trained on source)
    â†“
Deploy to new mine site
    â†“
Collect predictions + optional human feedback
    â†“
Continuously update model (online learning)
```

---

### 2.4 Multi-Source Domain Adaptation

#### 2.4.1 **Multiple Mine Sites as Sources**

**Concept**: Train on multiple mine sites to learn domain-agnostic features.

**Key Papers**:
- DomainNet - Peng et al., 2019
- MDDA (Multi-Domain Domain Adaptation) - Zhao et al., 2020

**Training Strategy**:
```
Mine Site A (labeled) â”€â”€â”
Mine Site B (labeled) â”€â”€â”¼â”€â”€â†’ Multi-source training â†’ Generalized model
Mine Site C (labeled) â”€â”€â”˜
                                    â†“
                            Deploy to Mine Site D (unlabeled)
```

**Domain Aggregation Methods**:
- Domain-specific batch normalization layers
- Attention-based domain weighting
- Meta-learning across sources

---

### 2.5 Unsupervised Domain Adaptation (UDA)

**Assumption**: Target domain has no labels.

**Key Methods**:

1. **Source-Free Domain Adaptation**
   - Adapt using only source model + target data
   - Protect source data privacy
   - Information maximization on target domain

2. **Universal Domain Adaptation**
   - Handle label set mismatch (target may have different classes)
   - "Unknown" class detection
   - Important if new mine sites have unique spoil types

---

## 3. Recommended Implementation Strategy

### 3.1 Phase 1: Foundation (Weeks 1-2)

#### 3.1.1 **Data Infrastructure**

**Dataset Organization**:
```
data/
â”œâ”€â”€ source_site/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ target_site_1/
â”‚   â”œâ”€â”€ unlabeled/
â”‚   â””â”€â”€ few_shot_labeled/ (optional)
â”œâ”€â”€ target_site_2/
â”‚   â””â”€â”€ unlabeled/
â””â”€â”€ metadata.json (domain labels, site characteristics)
```

**Implementation Tasks**:
- [ ] Create multi-domain dataset loader
- [ ] Add domain labels to CSV (site_id column)
- [ ] Implement domain-stratified data splitting
- [ ] Create visualization tools for domain statistics

**File**: `src/data/multi_domain_dataset.py`

---

#### 3.1.2 **Baseline Evaluation**

**Zero-Shot Transfer Evaluation**:
```python
# Train on source mine site
model.train(source_data)

# Evaluate on target mine sites (no adaptation)
for target_site in target_sites:
    metrics = model.evaluate(target_site)
    log_performance_drop(source_metrics, target_metrics)
```

**Metrics to Track**:
- Source domain accuracy (baseline)
- Target domain accuracy (zero-shot)
- Domain gap (performance difference)
- Per-class performance degradation
- Confusion matrix comparison

**Deliverable**: `results/baseline_domain_gap_analysis.pdf`

---

### 3.2 Phase 2: Core Domain Adaptation (Weeks 3-6)

#### 3.2.1 **Method 1: Adversarial Domain Adaptation (Priority 1)**

**Architecture Design**:

```python
class DomainAdaptiveMIL(nn.Module):
    def __init__(self):
        # Shared feature extractor
        self.feature_extractor = ViTR50()  # ViT-R50 backbone

        # MIL attention aggregator
        self.attention_aggregator = AttentionAggregator()

        # Task classifier (3 classes)
        self.class_classifier = ClassClassifier()

        # Domain discriminator (source vs target)
        self.domain_discriminator = DomainDiscriminator()

        # Gradient reversal layer
        self.grl = GradientReversalLayer(lambda_=1.0)

    def forward(self, bags, alpha):
        # Extract features
        patch_features = self.feature_extractor(bags)  # [B, N, 768]

        # Aggregate with attention
        bag_features, attention = self.attention_aggregator(patch_features)

        # Classification
        class_logits = self.class_classifier(bag_features)

        # Domain classification with gradient reversal
        domain_features = self.grl(bag_features, alpha)
        domain_logits = self.domain_discriminator(domain_features)

        return class_logits, domain_logits, attention
```

**Training Loop**:
```python
for epoch in range(num_epochs):
    for source_bags, target_bags in zip(source_loader, target_loader):
        # Forward pass
        source_class_logits, source_domain_logits, _ = model(source_bags, alpha)
        target_class_logits, target_domain_logits, _ = model(target_bags, alpha)

        # Classification loss (source only)
        class_loss = criterion(source_class_logits, source_labels)

        # Domain confusion loss
        source_domain_labels = torch.zeros(len(source_bags))
        target_domain_labels = torch.ones(len(target_bags))
        domain_loss = criterion(source_domain_logits, source_domain_labels) + \
                      criterion(target_domain_logits, target_domain_labels)

        # Total loss
        total_loss = class_loss + lambda_domain * domain_loss

        # Backpropagation
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        # Update lambda (GRL weight) - increase over time
        alpha = 2 / (1 + np.exp(-10 * epoch / num_epochs)) - 1
```

**Hyperparameters**:
- `lambda_domain`: 0.1 â†’ 1.0 (scheduled increase)
- `alpha`: 0.0 â†’ 1.0 (GRL strength, scheduled)
- Domain discriminator architecture: 512â†’256â†’128â†’1
- Learning rate: 1e-4 (feature extractor), 1e-3 (discriminator)

**Implementation Files**:
- `src/models/domain_adversarial_mil.py`
- `src/utils/gradient_reversal.py`
- `src/utils/domain_adversarial_training.py`

---

#### 3.2.2 **Method 2: MMD-Based Adaptation (Priority 1)**

**Loss Function**:

```python
def mmd_loss(source_features, target_features, kernel_mul=2.0, kernel_num=5):
    """
    Multi-kernel Maximum Mean Discrepancy
    """
    batch_size = source_features.size(0)

    # Compute kernels
    kernels = []
    for i in range(kernel_num):
        bandwidth = kernel_mul ** i
        kernels.append(gaussian_kernel(source_features, target_features, bandwidth))

    # Average over kernels
    mmd = sum(kernels) / len(kernels)

    return mmd

def gaussian_kernel(x, y, bandwidth):
    """Gaussian RBF kernel"""
    n_x = x.size(0)
    n_y = y.size(0)

    # Pairwise distances
    xx = torch.mm(x, x.t())
    yy = torch.mm(y, y.t())
    xy = torch.mm(x, y.t())

    # Compute kernel
    X2 = xx.diag().unsqueeze(0).expand_as(xx)
    Y2 = yy.diag().unsqueeze(0).expand_as(yy)

    K_xx = torch.exp(-bandwidth * (X2 + X2.t() - 2 * xx))
    K_yy = torch.exp(-bandwidth * (Y2 + Y2.t() - 2 * yy))
    K_xy = torch.exp(-bandwidth * (X2 + Y2.t() - 2 * xy))

    return K_xx.mean() + K_yy.mean() - 2 * K_xy.mean()
```

**Multi-Level MMD**:
```python
# Apply MMD at multiple levels
mmd_patch = mmd_loss(source_patch_features, target_patch_features)
mmd_bag = mmd_loss(source_bag_features, target_bag_features)
mmd_pile = mmd_loss(source_pile_features, target_pile_features)

total_mmd = alpha_patch * mmd_patch + alpha_bag * mmd_bag + alpha_pile * mmd_pile
total_loss = classification_loss + lambda_mmd * total_mmd
```

**Advantages for MIL**:
- Aligns distributions at all hierarchy levels
- No additional network components
- Stable training (no adversarial dynamics)

**Implementation Files**:
- `src/losses/mmd_loss.py`
- `src/models/mmd_domain_adaptive_mil.py`
- `src/utils/mmd_training.py`

---

#### 2.1.6 **Hybrid Approach: Combining Adversarial and MMD with Orthogonal Constraint**

**Motivation**: Combining adversarial domain adaptation (DANN) with MMD can provide complementary benefits - adversarial learning for fine-grained alignment and MMD for explicit distribution matching. However, this introduces a critical risk: **over-alignment (domain collapse)**.

**Risk: Over-Alignment (Domain Collapse)**

When both adversarial loss and MMD loss push features from different domains to align, they may align so aggressively that:
- **Class separability degrades**: Decision boundaries between classes become blurred
- **Feature collapse**: Different classes map to similar feature representations
- **Reduced discriminability**: The classifier can no longer distinguish between classes effectively

**Visual Illustration**:
```
Before Domain Adaptation:
Source: â—â—â— (Class 0)  â– â– â–  (Class 1)  â–²â–²â–² (Class 2)
Target: â—‹â—‹â—‹ (Class 0)  â–¡â–¡â–¡ (Class 1)  â–³â–³â–³ (Class 2)
â†“
Without Orthogonal Constraint (Over-aligned):
Both:   â—â—‹â—â—‹ (collapsed)  â– â–¡â– â–¡ (collapsed)  â–²â–³â–²â–³ (collapsed)
        [Domain-invariant but classes overlap!]
â†“
With Orthogonal Constraint (Proper alignment):
Both:   â—â—‹â—â—‹ (aligned, Class 0)  â– â–¡â– â–¡ (aligned, Class 1)  â–²â–³â–²â–³ (aligned, Class 2)
        [Domain-invariant AND classes remain separated]
```

**Solution: Orthogonal Constraint**

The key insight is to ensure that domain alignment (done by the domain discriminator) operates in a feature subspace **orthogonal** to the class discrimination subspace (used by the class classifier). This prevents domain alignment from rotating or distorting the class decision boundaries.

**Mathematical Formulation**:

```python
# Classification loss (standard cross-entropy)
L_cls = CrossEntropy(class_classifier(features), labels)

# Adversarial domain loss (GRL)
L_adv = CrossEntropy(domain_discriminator(GRL(features)), domain_labels)

# MMD loss (distribution matching)
L_mmd = MMD(source_features, target_features)

# Orthogonal constraint (key addition!)
# Ensure classifier weights and discriminator weights are orthogonal
W_cls = class_classifier.weight  # Shape: [num_classes, feature_dim]
W_domain = domain_discriminator.weight  # Shape: [1, feature_dim]

# Compute orthogonality: should be close to zero
L_orthogonal = torch.norm(torch.mm(W_cls, W_domain.T))

# Alternative: Cosine similarity should be close to zero
# L_orthogonal = torch.abs(F.cosine_similarity(W_cls, W_domain.expand_as(W_cls), dim=1).mean())

# Total loss with all components
L_total = L_cls + Î»_adv * L_adv + Î»_mmd * L_mmd + Î»_orth * L_orthogonal
```

**Implementation**:

```python
class HybridDomainAdaptiveMIL(nn.Module):
    """
    Combines Adversarial + MMD + Orthogonal Constraint
    for robust domain adaptation without over-alignment
    """
    def __init__(self, feature_dim=768, num_classes=3):
        super().__init__()

        # Feature extractor
        self.feature_extractor = ViTR50()

        # MIL attention aggregator
        self.attention_aggregator = AttentionAggregator()

        # Class classifier
        self.class_classifier = nn.Sequential(
            nn.Linear(feature_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

        # Domain discriminator
        self.domain_discriminator = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 1)
        )

        # Gradient reversal layer
        self.grl = GradientReversalLayer()

    def forward(self, bags, alpha=1.0):
        # Extract features
        patch_features = self.feature_extractor(bags)

        # Aggregate with attention
        bag_features, attention = self.attention_aggregator(patch_features)

        # Class prediction
        class_logits = self.class_classifier(bag_features)

        # Domain prediction (with gradient reversal)
        domain_features = self.grl(bag_features, alpha)
        domain_logits = self.domain_discriminator(domain_features)

        return class_logits, domain_logits, bag_features, attention

    def get_orthogonal_loss(self):
        """
        Compute orthogonality constraint between classifier and discriminator weights
        """
        # Get weights from final linear layers
        cls_weight = self.class_classifier[-1].weight  # [num_classes, 512]
        domain_weight = self.domain_discriminator[-1].weight  # [1, 256]

        # If dimensions don't match, use earlier layers
        # Option 1: Use first layer weights
        cls_weight = self.class_classifier[0].weight  # [512, feature_dim]
        domain_weight = self.domain_discriminator[0].weight  # [256, feature_dim]

        # Project to common dimension for comparison
        # Compute Gram matrix to measure alignment
        cls_norm = F.normalize(cls_weight, p=2, dim=1)  # Normalize rows
        domain_norm = F.normalize(domain_weight, p=2, dim=1)

        # Orthogonality: minimize inner product
        # High value = vectors are aligned (BAD)
        # Low value = vectors are orthogonal (GOOD)
        orthogonality = torch.mm(cls_norm, domain_norm.T)
        loss = torch.norm(orthogonality)  # Minimize this

        return loss


def train_hybrid_domain_adaptation(model, source_loader, target_loader,
                                   lambda_adv=1.0, lambda_mmd=0.5, lambda_orth=0.1):
    """
    Training loop with hybrid approach and orthogonal constraint
    """
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

    for epoch in range(num_epochs):
        for (source_bags, source_labels), (target_bags, _) in zip(source_loader, target_loader):

            # Forward pass on source
            source_class_logits, source_domain_logits, source_features, _ = model(source_bags, alpha)

            # Forward pass on target
            target_class_logits, target_domain_logits, target_features, _ = model(target_bags, alpha)

            # 1. Classification loss (source only)
            loss_cls = F.cross_entropy(source_class_logits, source_labels)

            # 2. Adversarial domain loss
            source_domain_labels = torch.zeros(len(source_bags), device=device)
            target_domain_labels = torch.ones(len(target_bags), device=device)

            loss_adv = F.binary_cross_entropy_with_logits(
                source_domain_logits.squeeze(), source_domain_labels
            ) + F.binary_cross_entropy_with_logits(
                target_domain_logits.squeeze(), target_domain_labels
            )

            # 3. MMD loss
            loss_mmd = mmd_loss(source_features, target_features)

            # 4. Orthogonal constraint (CRITICAL!)
            loss_orth = model.get_orthogonal_loss()

            # Total loss
            total_loss = loss_cls + lambda_adv * loss_adv + lambda_mmd * loss_mmd + lambda_orth * loss_orth

            # Backpropagation
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

            # Update GRL alpha (schedule from 0 to 1)
            alpha = 2 / (1 + np.exp(-10 * epoch / num_epochs)) - 1

            # Logging
            if batch_idx % 10 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}")
                print(f"  L_cls: {loss_cls:.4f}")
                print(f"  L_adv: {loss_adv:.4f}")
                print(f"  L_mmd: {loss_mmd:.4f}")
                print(f"  L_orth: {loss_orth:.4f} â† Monitor this!")
                print(f"  Total: {total_loss:.4f}")
```

**Hyperparameter Tuning**:

| Parameter | Recommended Range | Effect |
|-----------|------------------|---------|
| `Î»_adv` | 0.5 - 2.0 | Higher = stronger adversarial alignment (risk: over-alignment) |
| `Î»_mmd` | 0.1 - 1.0 | Higher = stronger distribution matching (risk: over-alignment) |
| `Î»_orth` | 0.05 - 0.5 | Higher = stronger orthogonality constraint (prevents over-alignment) |

**Tuning Strategy**:
1. Start with `Î»_orth = 0.1` (moderate constraint)
2. If target accuracy improves but source accuracy drops â†’ increase `Î»_orth`
3. If domain gap remains large â†’ increase `Î»_adv` and `Î»_mmd`, but also increase `Î»_orth` proportionally
4. Monitor the orthogonality loss: should decrease over training but not collapse to zero

**Monitoring Metrics for Over-Alignment**:
```python
# During validation
def check_over_alignment(model, source_data, target_data):
    """Diagnostic checks for over-alignment"""

    # 1. Source domain accuracy should NOT drop
    source_acc = evaluate(model, source_data)
    if source_acc < baseline_source_acc * 0.95:
        print("âš ï¸ WARNING: Source accuracy dropped - possible over-alignment!")

    # 2. Class separation in feature space
    features, labels = extract_features(model, source_data)
    within_class_var = compute_within_class_variance(features, labels)
    between_class_var = compute_between_class_variance(features, labels)
    separation_ratio = between_class_var / within_class_var

    if separation_ratio < 2.0:
        print("âš ï¸ WARNING: Low class separation - possible feature collapse!")

    # 3. Feature diversity (should not collapse to single point)
    feature_std = torch.std(features, dim=0).mean()
    if feature_std < 0.1:
        print("âš ï¸ WARNING: Low feature diversity - possible collapse!")

    # 4. Orthogonality metric
    orth_loss = model.get_orthogonal_loss()
    if orth_loss > 0.5:
        print("âš ï¸ WARNING: High orthogonality loss - classifier and discriminator not orthogonal!")
```

**When to Use This Hybrid Approach**:

âœ… **Use when**:
- Domain shift is severe (large covariate shift)
- You want complementary alignment (adversarial + statistical)
- You have sufficient computational resources
- Source domain performance must be maintained

âŒ **Avoid when**:
- Single method (DANN or MMD alone) already works well
- Limited computational budget (3 losses to compute)
- Small dataset (risk of overfitting to regularization terms)

**Expected Benefits**:
- **Better alignment**: Combining adversarial + MMD often achieves 2-5% better target accuracy than single methods
- **Maintained source performance**: Orthogonal constraint prevents catastrophic forgetting
- **Robust to hyperparameters**: Less sensitive to exact values of Î»_adv and Î»_mmd
- **Reduced domain gap**: Often achieves <5% domain gap (vs. 10-15% for single methods)

**Implementation Files**:
- `src/models/hybrid_domain_adaptive_mil.py`
- `src/losses/orthogonal_constraint.py`
- `src/training/hybrid_training.py`

---

#### 3.2.3 **Method 3: Contrastive Domain Adaptation (Priority 2)**

**Contrastive Loss Design**:

```python
class ContrastiveDomainLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, features, labels, domains):
        """
        features: [B, D] normalized features
        labels: [B] class labels
        domains: [B] domain labels (0=source, 1=target)
        """
        # Compute similarity matrix
        sim_matrix = torch.matmul(features, features.T) / self.temperature

        # Create positive mask (same class, different domain preferred)
        label_mask = labels.unsqueeze(0) == labels.unsqueeze(1)
        domain_mask = domains.unsqueeze(0) != domains.unsqueeze(1)
        positive_mask = label_mask & domain_mask  # Same class, different domain

        # Create negative mask (different class)
        negative_mask = ~label_mask

        # Compute loss
        exp_sim = torch.exp(sim_matrix)

        # Positive pairs: same class across domains
        pos_sim = (exp_sim * positive_mask).sum(1)

        # Negative pairs: different class
        neg_sim = (exp_sim * negative_mask).sum(1)

        # Contrastive loss
        loss = -torch.log(pos_sim / (pos_sim + neg_sim + 1e-8))

        return loss.mean()
```

**Training Strategy**:
```python
# Projection head for contrastive learning
projection_head = nn.Sequential(
    nn.Linear(768, 512),
    nn.ReLU(),
    nn.Linear(512, 256)
)

# Training loop
for source_bags, target_bags in loader:
    # Extract features
    source_features = model.feature_extractor(source_bags)
    target_features = model.feature_extractor(target_bags)

    # Project for contrastive learning
    source_proj = projection_head(source_features)
    target_proj = projection_head(target_features)

    # Combine
    all_features = torch.cat([source_proj, target_proj])
    all_labels = torch.cat([source_labels, pseudo_target_labels])
    all_domains = torch.cat([
        torch.zeros(len(source_bags)),
        torch.ones(len(target_bags))
    ])

    # Contrastive loss
    contrastive_loss = criterion(all_features, all_labels, all_domains)

    # Classification loss
    class_loss = ce_loss(source_class_logits, source_labels)

    # Total loss
    total_loss = class_loss + lambda_contrast * contrastive_loss
```

**Benefits**:
- Learns discriminative cross-domain features
- Reduces intra-class domain gap
- Maintains inter-class separation

**Implementation Files**:
- `src/losses/contrastive_domain_loss.py`
- `src/models/contrastive_domain_adaptive_mil.py`

---

### 3.3 Phase 3: Advanced Techniques (Weeks 7-9)

#### 3.3.1 **Self-Training with Pseudo-Labels**

**Pipeline**:

```python
class SelfTrainingPipeline:
    def __init__(self, base_model, confidence_threshold=0.9):
        self.teacher_model = base_model
        self.student_model = copy.deepcopy(base_model)
        self.confidence_threshold = confidence_threshold

    def train(self, source_data, target_data, num_iterations=5):
        for iteration in range(num_iterations):
            print(f"Self-training iteration {iteration+1}")

            # 1. Generate pseudo-labels on target domain
            pseudo_labels, confidences = self.generate_pseudo_labels(target_data)

            # 2. Select high-confidence samples
            high_conf_mask = confidences > self.confidence_threshold
            selected_target_data = target_data[high_conf_mask]
            selected_pseudo_labels = pseudo_labels[high_conf_mask]

            # 3. Combine source and pseudo-labeled target data
            combined_data = source_data + (selected_target_data, selected_pseudo_labels)

            # 4. Train student model
            self.student_model.train(combined_data)

            # 5. Update teacher with EMA
            self.teacher_model = self.ema_update(self.teacher_model, self.student_model)

            # 6. Lower confidence threshold (progressive)
            self.confidence_threshold *= 0.95

    def generate_pseudo_labels(self, target_data):
        """Generate pseudo-labels using teacher model"""
        self.teacher_model.eval()
        with torch.no_grad():
            logits = self.teacher_model(target_data)
            probs = F.softmax(logits, dim=1)
            confidences, pseudo_labels = probs.max(dim=1)
        return pseudo_labels, confidences

    def ema_update(self, teacher, student, alpha=0.999):
        """Exponential moving average update"""
        for teacher_param, student_param in zip(teacher.parameters(), student.parameters()):
            teacher_param.data = alpha * teacher_param.data + (1 - alpha) * student_param.data
        return teacher
```

**Progressive Thresholding Schedule**:
```
Iteration 1: threshold = 0.95 (very high confidence only)
Iteration 2: threshold = 0.90
Iteration 3: threshold = 0.85
Iteration 4: threshold = 0.80
Iteration 5: threshold = 0.75
```

**Class-Balanced Sampling**:
```python
# Ensure balanced pseudo-labeled samples per class
def balanced_pseudo_label_sampling(pseudo_labels, confidences, samples_per_class=100):
    selected_indices = []
    for class_id in range(num_classes):
        class_mask = pseudo_labels == class_id
        class_confidences = confidences[class_mask]

        # Select top-k confident samples
        top_k_indices = torch.topk(class_confidences, k=samples_per_class).indices
        selected_indices.extend(top_k_indices)

    return selected_indices
```

**Implementation Files**:
- `src/training/self_training.py`
- `src/training/pseudo_labeling.py`

---

#### 3.3.2 **Mining-Specific Augmentation**

**Color Space Transformations**:

```python
class MiningColorAugmentation:
    """Simulate color variations across mine sites"""

    def __init__(self):
        # Different mineral compositions â†’ different color profiles
        self.color_shifts = {
            'iron_rich': {'r': 1.2, 'g': 0.9, 'b': 0.8},      # Reddish
            'clay_rich': {'r': 1.1, 'g': 1.1, 'b': 0.9},       # Yellowish
            'limestone': {'r': 1.05, 'g': 1.05, 'b': 1.05},    # Whitish
            'coal': {'r': 0.8, 'g': 0.8, 'b': 0.8},            # Darker
        }

    def apply(self, image, mineral_type=None):
        """Apply mineral-specific color transformation"""
        if mineral_type is None:
            mineral_type = random.choice(list(self.color_shifts.keys()))

        shifts = self.color_shifts[mineral_type]

        # Apply to RGB channels
        image[:, :, 0] *= shifts['r']
        image[:, :, 1] *= shifts['g']
        image[:, :, 2] *= shifts['b']

        return torch.clamp(image, 0, 1)

class IlluminationAugmentation:
    """Simulate different lighting conditions"""

    def apply(self, image):
        # Simulate different times of day
        brightness_factor = random.uniform(0.6, 1.4)

        # Simulate shadow patterns (localized darkening)
        shadow_mask = self.generate_shadow_mask(image.shape)

        augmented = image * brightness_factor * shadow_mask
        return torch.clamp(augmented, 0, 1)

    def generate_shadow_mask(self, shape):
        """Generate realistic shadow patterns"""
        # Use Perlin noise for natural-looking shadows
        mask = perlin_noise(shape, scale=50)
        mask = (mask + 1) / 2  # Normalize to [0, 1]
        mask = mask * 0.5 + 0.5  # Range [0.5, 1.0]
        return mask

class WeatheringAugmentation:
    """Simulate weathering and oxidation patterns"""

    def apply(self, image):
        # Add texture noise (weathering patterns)
        noise = torch.randn_like(image) * 0.05

        # Apply spatially-varying oxidation (reddish tint in patches)
        oxidation_mask = self.generate_oxidation_mask(image.shape)
        oxidation_tint = torch.tensor([1.3, 0.9, 0.7])  # Reddish

        augmented = image + noise
        augmented = augmented * (1 - oxidation_mask) + \
                    (image * oxidation_tint) * oxidation_mask

        return torch.clamp(augmented, 0, 1)
```

**Integration into Training**:
```python
augmentation_pipeline = Compose([
    MiningColorAugmentation(),
    IlluminationAugmentation(),
    WeatheringAugmentation(),
    RandomRotation(15),
    RandomZoom(0.9, 2.5),
    RandomHorizontalFlip(),
])
```

**Implementation Files**:
- `src/augmentation/mining_augmentation.py`
- `src/augmentation/color_transformations.py`

---

#### 3.3.3 **Test-Time Adaptation**

**Entropy Minimization**:

```python
class TestTimeAdaptation:
    def __init__(self, model):
        self.model = model
        self.adapted_model = copy.deepcopy(model)

        # Only adapt batch normalization and attention layers
        self.configure_trainable_params()

    def configure_trainable_params(self):
        """Freeze all except BN and attention"""
        for param in self.adapted_model.parameters():
            param.requires_grad = False

        # Enable BN layers
        for module in self.adapted_model.modules():
            if isinstance(module, nn.BatchNorm2d):
                module.requires_grad = True
                module.track_running_stats = False  # Use batch statistics

        # Enable attention aggregator
        for param in self.adapted_model.attention_aggregator.parameters():
            param.requires_grad = True

    def adapt(self, test_batch, num_steps=5, lr=1e-3):
        """Adapt model to test batch"""
        optimizer = torch.optim.Adam(
            filter(lambda p: p.requires_grad, self.adapted_model.parameters()),
            lr=lr
        )

        for step in range(num_steps):
            # Forward pass
            logits = self.adapted_model(test_batch)
            probs = F.softmax(logits, dim=1)

            # Entropy minimization loss
            entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1).mean()

            # Diversity regularization (prevent collapse)
            avg_probs = probs.mean(dim=0)
            diversity = (avg_probs * torch.log(avg_probs + 1e-8)).sum()

            # Total loss
            loss = entropy + 0.1 * diversity

            # Update
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        return self.adapted_model

    def predict(self, test_data, adapt_batch_size=32):
        """Predict with test-time adaptation"""
        predictions = []

        # Process in batches
        for batch in DataLoader(test_data, batch_size=adapt_batch_size):
            # Adapt to this batch
            adapted_model = self.adapt(batch)

            # Predict
            with torch.no_grad():
                logits = adapted_model(batch)
                preds = logits.argmax(dim=1)

            predictions.extend(preds)

        return predictions
```

**Implementation Files**:
- `src/inference/test_time_adaptation.py`

---

### 3.4 Phase 4: Multi-Source Learning (Weeks 10-11)

#### 3.4.1 **Multi-Domain Dataset Preparation**

**Data Collection Strategy**:
```
Collect data from â‰¥3 diverse mine sites:
- Site A: Iron-rich geology
- Site B: Clay-rich geology
- Site C: Limestone-dominant
- Site D: Mixed geology (target for testing)
```

**Domain-Specific Batch Normalization**:

```python
class DomainSpecificBatchNorm(nn.Module):
    """Separate BN statistics for each domain"""

    def __init__(self, num_features, num_domains):
        super().__init__()
        self.num_domains = num_domains

        # Create separate BN layers for each domain
        self.bn_layers = nn.ModuleList([
            nn.BatchNorm2d(num_features) for _ in range(num_domains)
        ])

    def forward(self, x, domain_id):
        """
        x: input features [B, C, H, W]
        domain_id: domain identifier [B]
        """
        # Apply domain-specific BN
        output = torch.zeros_like(x)
        for domain in range(self.num_domains):
            mask = domain_id == domain
            if mask.any():
                output[mask] = self.bn_layers[domain](x[mask])

        return output
```

**Domain Attention Network**:

```python
class DomainAttentionAggregator(nn.Module):
    """Learn to weight different source domains"""

    def __init__(self, feature_dim, num_domains):
        super().__init__()
        self.domain_attention = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_domains),
            nn.Softmax(dim=1)
        )

    def forward(self, features, domain_features):
        """
        features: [B, D] input features
        domain_features: [num_domains, D] domain-specific features
        """
        # Compute attention weights over domains
        attention_weights = self.domain_attention(features)  # [B, num_domains]

        # Weighted combination of domain-specific features
        weighted_features = torch.einsum('bd,df->bf', attention_weights, domain_features)

        return weighted_features, attention_weights
```

**Implementation Files**:
- `src/models/multi_domain_mil.py`
- `src/data/multi_domain_loader.py`

---

### 3.5 Phase 5: Evaluation & Deployment (Weeks 12-13)

#### 3.5.1 **Comprehensive Evaluation Protocol**

**Cross-Domain Evaluation Metrics**:

```python
class DomainAdaptationEvaluator:
    """Comprehensive evaluation for domain adaptation"""

    def evaluate(self, model, source_data, target_data):
        metrics = {}

        # 1. Source domain performance (should remain high)
        metrics['source_accuracy'] = self.compute_accuracy(model, source_data)

        # 2. Target domain performance (primary goal)
        metrics['target_accuracy'] = self.compute_accuracy(model, target_data)

        # 3. Domain gap (reduction)
        metrics['domain_gap'] = metrics['source_accuracy'] - metrics['target_accuracy']

        # 4. Class-wise performance
        metrics['source_per_class_f1'] = self.compute_per_class_f1(model, source_data)
        metrics['target_per_class_f1'] = self.compute_per_class_f1(model, target_data)

        # 5. Feature distribution alignment
        metrics['mmd_distance'] = self.compute_mmd(
            model.extract_features(source_data),
            model.extract_features(target_data)
        )

        # 6. Domain confusion (ideal: 50% for perfect alignment)
        metrics['domain_confusion'] = self.compute_domain_classification_accuracy(
            model.extract_features(source_data),
            model.extract_features(target_data)
        )

        # 7. Calibration error
        metrics['target_ece'] = self.expected_calibration_error(model, target_data)

        return metrics

    def expected_calibration_error(self, model, data, num_bins=10):
        """Measure prediction confidence calibration"""
        predictions, confidences, true_labels = [], [], []

        with torch.no_grad():
            for batch, labels in data:
                logits = model(batch)
                probs = F.softmax(logits, dim=1)
                conf, pred = probs.max(dim=1)

                predictions.extend(pred.cpu().numpy())
                confidences.extend(conf.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())

        # Bin predictions by confidence
        ece = 0
        for bin_idx in range(num_bins):
            bin_lower = bin_idx / num_bins
            bin_upper = (bin_idx + 1) / num_bins

            in_bin = [(bin_lower <= c < bin_upper) for c in confidences]
            if sum(in_bin) == 0:
                continue

            bin_accuracy = np.mean([p == l for p, l, ib in
                                   zip(predictions, true_labels, in_bin) if ib])
            bin_confidence = np.mean([c for c, ib in
                                     zip(confidences, in_bin) if ib])

            ece += (bin_accuracy - bin_confidence) ** 2 * sum(in_bin) / len(predictions)

        return ece
```

**Visualization Tools**:

```python
def visualize_domain_adaptation(source_features, target_features, method='tsne'):
    """Visualize feature distributions before/after adaptation"""
    from sklearn.manifold import TSNE
    import matplotlib.pyplot as plt

    # Reduce dimensions
    if method == 'tsne':
        embeddings = TSNE(n_components=2).fit_transform(
            np.vstack([source_features, target_features])
        )

    # Plot
    plt.figure(figsize=(10, 5))

    # Source domain
    plt.scatter(embeddings[:len(source_features), 0],
               embeddings[:len(source_features), 1],
               c='blue', label='Source', alpha=0.5)

    # Target domain
    plt.scatter(embeddings[len(source_features):, 0],
               embeddings[len(source_features):, 1],
               c='red', label='Target', alpha=0.5)

    plt.legend()
    plt.title('Feature Distribution Visualization')
    plt.savefig('results/domain_adaptation_visualization.png')
```

**Implementation Files**:
- `src/evaluation/domain_evaluation.py`
- `src/visualization/domain_visualization.py`

---

#### 3.5.2 **Model Selection Criteria**

**Primary Metrics** (Target Domain):
1. **Accuracy**: Overall classification accuracy
2. **F1-Score**: Weighted F1 across classes
3. **Worst-Class Performance**: Minimum per-class F1 (ensure no class neglect)

**Secondary Metrics**:
1. **Source Domain Performance**: Should not degrade significantly
2. **Domain Gap**: Source accuracy - Target accuracy (should be minimized)
3. **Calibration**: Expected Calibration Error (ECE) < 0.1
4. **Inference Speed**: Maintain real-time capability

**Selection Protocol**:
```python
def select_best_model(models, source_data, target_data):
    """Select best domain-adapted model"""
    results = []

    for model_name, model in models.items():
        # Evaluate
        target_acc = evaluate_accuracy(model, target_data)
        target_f1 = evaluate_f1(model, target_data)
        worst_class_f1 = min(evaluate_per_class_f1(model, target_data))
        source_acc = evaluate_accuracy(model, source_data)
        domain_gap = source_acc - target_acc

        # Composite score
        score = (
            0.4 * target_acc +
            0.3 * target_f1 +
            0.2 * worst_class_f1 +
            0.1 * (1 - domain_gap)  # Prefer smaller gap
        )

        results.append({
            'model': model_name,
            'score': score,
            'target_accuracy': target_acc,
            'target_f1': target_f1,
            'worst_class_f1': worst_class_f1,
            'domain_gap': domain_gap
        })

    # Sort by score
    results.sort(key=lambda x: x['score'], reverse=True)

    return results
```

---

#### 3.5.3 **Deployment Strategy**

**Deployment Pipeline**:

```
1. Initial Model Selection
   - Train all methods on source domain
   - Evaluate on held-out target domain validation set
   - Select top 3 performing methods

2. Ensemble Strategy (Optional)
   - Combine predictions from top 3 models
   - Weighted voting based on validation performance
   - Confidence-based selection (use most confident model per sample)

3. Active Learning Loop
   - Deploy model to new mine site
   - Identify low-confidence predictions
   - Request human labels for uncertain samples
   - Periodically retrain with new labels

4. Continuous Monitoring
   - Track prediction confidence distribution
   - Alert when confidence drops (domain shift detection)
   - Trigger retraining or adaptation
```

**Confidence-Based Routing**:

```python
class AdaptiveModelRouter:
    """Route samples to best model based on confidence"""

    def __init__(self, models):
        self.models = models  # Dictionary of models

    def predict(self, sample):
        # Get predictions from all models
        predictions = {}
        confidences = {}

        for name, model in self.models.items():
            logits = model(sample)
            probs = F.softmax(logits, dim=1)
            conf, pred = probs.max(dim=1)

            predictions[name] = pred
            confidences[name] = conf

        # Select most confident model
        best_model = max(confidences, key=confidences.get)

        return predictions[best_model], confidences[best_model], best_model
```

---

## 4. Implementation Roadmap

### 4.1 Development Timeline (13 Weeks)

| Week | Phase | Tasks | Deliverables |
|------|-------|-------|--------------|
| 1-2 | Foundation | Data infrastructure, baseline evaluation | Multi-domain dataset, baseline report |
| 3-4 | Core DA #1 | Adversarial domain adaptation | DANN implementation, training pipeline |
| 5-6 | Core DA #2 | MMD-based adaptation, contrastive learning | MMD implementation, results comparison |
| 7-8 | Advanced #1 | Self-training with pseudo-labels | Self-training pipeline, iteration results |
| 9 | Advanced #2 | Mining-specific augmentation, test-time adaptation | Augmentation module, TTA implementation |
| 10-11 | Multi-source | Multi-domain training, domain attention | Multi-source model, evaluation |
| 12 | Evaluation | Comprehensive evaluation, model selection | Performance report, visualizations |
| 13 | Deployment | Deployment pipeline, monitoring | Production-ready model, documentation |

### 4.2 Priority Ranking

**Tier 1 (Must Implement)**:
1. âœ… Adversarial Domain Adaptation (DANN)
2. âœ… MMD-based Adaptation
3. âœ… Self-Training with Pseudo-Labels
4. âœ… Mining-Specific Augmentation

**Tier 2 (High Value)**:
5. âš¡ Contrastive Domain Adaptation
6. âš¡ Test-Time Adaptation
7. âš¡ Multi-Source Domain Learning

**Tier 3 (Experimental)**:
8. ğŸ”¬ Meta-Learning for Few-Shot Adaptation
9. ğŸ”¬ Style Transfer for Domain Bridging
10. ğŸ”¬ Universal Domain Adaptation (for unknown classes)

---

## 5. Expected Outcomes

### 5.1 Performance Targets

**Baseline (Current)**:
- Source domain accuracy: 83-88%
- Target domain accuracy (zero-shot): ~40-60% (estimated)
- Domain gap: 25-45%

**Target After Domain Adaptation**:
- Source domain accuracy: â‰¥80% (maintain performance)
- Target domain accuracy: â‰¥75% (significant improvement)
- Domain gap: <10% (strong alignment)

**Per-Method Expected Improvements**:

| Method | Target Domain Accuracy | Domain Gap | Training Time | Inference Speed |
|--------|----------------------|------------|---------------|-----------------|
| Baseline (Zero-shot) | 50% | 35% | - | Fast |
| DANN | 72-75% | 10-15% | 1.5x | Fast |
| MMD | 70-73% | 12-17% | 1.3x | Fast |
| Self-Training | 75-78% | 8-12% | 2.0x | Fast |
| Contrastive DA | 73-76% | 10-14% | 1.4x | Fast |
| Test-Time Adapt | 68-71% | 15-20% | 1.0x | Slow (adaptive) |
| Multi-Source | 76-80% | 5-10% | 2.5x | Fast |
| **Ensemble** | **78-82%** | **5-8%** | - | Medium |

### 5.2 Success Criteria

âœ… **Primary Success**:
- Target domain accuracy > 75%
- All classes achieve F1 > 0.65
- Domain gap < 10%

âœ… **Secondary Success**:
- Source domain performance maintained (>80%)
- Inference time < 2x baseline
- Model size increase < 50%

âœ… **Stretch Goals**:
- Target domain accuracy > 80%
- Generalization to unseen 4th mine site > 70%
- Active learning reduces labeling effort by 80%

---

## 6. Resources & Requirements

### 6.1 Data Requirements

**Source Domain** (Training Site):
- Minimum: 1000 labeled images (current dataset)
- Optimal: 2000+ labeled images across all classes
- Balanced class distribution preferred

**Target Domain** (New Mine Sites):
- **Unsupervised DA**: 500-1000 unlabeled images per site
- **Few-Shot DA**: 10-50 labeled samples per class
- **Semi-Supervised DA**: 100-200 labeled + 500 unlabeled per site

**Multi-Source Learning**:
- 3-5 different source mine sites
- 500+ labeled images per site

### 6.2 Computational Requirements

**Training**:
- GPU: NVIDIA A100 (40GB) or V100 (32GB)
- RAM: 64GB+
- Storage: 500GB (datasets + checkpoints)
- Training time: 2-3 days per method

**Inference**:
- GPU: NVIDIA T4 or better
- RAM: 16GB
- Latency: <100ms per image (batch processing)

### 6.3 Software Dependencies

**New Libraries**:
```
# Domain adaptation
torch-domainadapt>=0.1.0

# Advanced augmentation
albumentations>=1.3.0

# Contrastive learning
pytorch-metric-learning>=1.7.0

# Visualization
umap-learn>=0.5.0
seaborn>=0.12.0

# Experiment tracking
wandb>=0.13.0
tensorboard>=2.11.0
```

---

## 7. Risk Mitigation

### 7.1 Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Insufficient target domain data | Medium | High | Use unsupervised DA methods (DANN, MMD) |
| Negative transfer (worse performance) | Low | High | Maintain source validation, early stopping |
| Overfitting to target domain | Medium | Medium | Regularization, hold-out target validation |
| Increased training time | High | Low | Start with efficient methods (MMD), parallelize |
| Label noise in pseudo-labels | High | Medium | Confidence thresholding, ensemble filtering |
| **Over-alignment (Domain Collapse)** | **Medium-High** | **Critical** | **Orthogonal constraint, monitor source accuracy** |

### 7.2 Fallback Strategies

1. **If DA methods underperform**:
   - Fall back to strong augmentation + transfer learning
   - Use ensemble of source-trained models
   - Request more target domain labels

2. **If target domain data is limited**:
   - Prioritize few-shot meta-learning
   - Use style transfer for data augmentation
   - Active learning for efficient labeling

3. **If computational resources are limited**:
   - Start with MMD (no discriminator)
   - Use smaller batch sizes with gradient accumulation
   - Pre-extract features, train only top layers

---

### 7.3 Critical Risk: Over-Alignment (Domain Collapse)

**Problem Statement**:

When combining multiple domain adaptation techniques (especially Adversarial + MMD), there is a critical risk that the model will align source and target domains **too aggressively**, causing features from different classes to collapse into similar representations. This phenomenon is called **over-alignment** or **domain collapse**.

**Why This Happens**:

1. **Adversarial Loss** pushes features to confuse the domain discriminator
   - Drives source and target features closer together
   - No inherent constraint to preserve class boundaries

2. **MMD Loss** minimizes statistical distance between domains
   - Explicitly matches feature distributions
   - Can inadvertently match across class boundaries

3. **Combined Effect** is multiplicative:
   - Both losses work in the same direction (alignment)
   - No opposing force to maintain class separability
   - Decision boundaries can rotate, shrink, or disappear

**Symptoms of Over-Alignment**:

| Symptom | How to Detect | Healthy Range | Danger Zone |
|---------|---------------|---------------|-------------|
| Source accuracy drop | Compare to baseline | >95% of baseline | <90% of baseline |
| Target accuracy plateau | Monitor validation curve | Steady improvement | Plateaus early then drops |
| Class confusion increase | Confusion matrix off-diagonals | <10% per class | >20% per class |
| Feature variance collapse | `torch.std(features).mean()` | >0.3 | <0.1 |
| Class separation ratio | `between_class_var / within_class_var` | >3.0 | <2.0 |
| Orthogonality loss | `â€–W_cls^T W_domainâ€–` | <0.3 | >0.5 |

**Detailed Mitigation Strategies**:

#### Strategy 1: Orthogonal Constraint (Recommended)

**Concept**: Force classifier and discriminator to operate in orthogonal subspaces.

```python
# Add to loss function
L_orthogonal = torch.norm(torch.mm(W_cls, W_domain.T))
L_total = L_cls + Î»_adv*L_adv + Î»_mmd*L_mmd + Î»_orth*L_orthogonal
```

**Benefits**:
- Directly prevents domain alignment from affecting class boundaries
- Mathematically grounded (subspace orthogonality)
- Small computational overhead
- Effective across different domain gaps

**Hyperparameter Guidance**:
- Start with `Î»_orth = 0.1`
- If source accuracy drops: increase to 0.2-0.5
- If domain gap remains large: keep at 0.1, increase `Î»_adv` and `Î»_mmd` instead

#### Strategy 2: Conditional Domain Adaptation

**Concept**: Align domains **within each class separately**, not globally.

```python
# Instead of global alignment:
L_adv = adversarial_loss(all_features)  # BAD - ignores classes

# Use class-conditional alignment:
for class_id in range(num_classes):
    source_class_features = source_features[source_labels == class_id]
    target_class_features = target_features[pseudo_labels == class_id]
    L_adv += adversarial_loss(source_class_features, target_class_features)
```

**Benefits**:
- Aligns same-class features across domains
- Preserves inter-class separation by design
- Used in CDAN (Conditional Domain Adversarial Network)

**Challenges**:
- Requires pseudo-labels for target domain
- More complex implementation
- Higher computational cost (per-class alignment)

#### Strategy 3: Progressive Adaptation with Early Stopping

**Concept**: Start adaptation gradually, stop before over-alignment occurs.

```python
# Schedule adaptation strength
def get_adaptation_weight(epoch, max_epochs):
    # Gradual increase, then plateau
    if epoch < max_epochs * 0.5:
        return (epoch / (max_epochs * 0.5)) * 1.0  # Increase to 1.0
    else:
        return 1.0  # Hold constant

lambda_adv = get_adaptation_weight(epoch, max_epochs) * base_lambda_adv

# Early stopping on SOURCE validation set
if source_val_accuracy < best_source_accuracy * 0.95:
    print("Source accuracy dropped - stopping adaptation")
    break
```

**Benefits**:
- Simple to implement
- Catches over-alignment before catastrophic
- Works with any DA method

**Limitations**:
- May stop before reaching optimal target performance
- Requires careful monitoring
- Suboptimal if domain gap is large

#### Strategy 4: Class-Aware Regularization

**Concept**: Explicitly preserve class structure during adaptation.

```python
def class_separation_loss(features, labels):
    """
    Encourage large between-class distance, small within-class distance
    """
    num_classes = labels.max() + 1

    # Compute class centroids
    centroids = []
    for c in range(num_classes):
        class_features = features[labels == c]
        centroids.append(class_features.mean(dim=0))
    centroids = torch.stack(centroids)  # [num_classes, feature_dim]

    # Within-class compactness
    within_class_loss = 0
    for c in range(num_classes):
        class_features = features[labels == c]
        distances = torch.norm(class_features - centroids[c], dim=1)
        within_class_loss += distances.mean()

    # Between-class separation
    centroid_distances = torch.cdist(centroids, centroids)
    # Exclude diagonal (distance to self)
    mask = ~torch.eye(num_classes, dtype=torch.bool)
    between_class_loss = -centroid_distances[mask].mean()  # Negative = maximize

    return within_class_loss + between_class_loss

# Add to total loss
L_class_structure = class_separation_loss(source_features, source_labels)
L_total = L_cls + Î»_adv*L_adv + Î»_mmd*L_mmd + Î»_structure*L_class_structure
```

**Benefits**:
- Explicitly maintains class structure
- Works with any DA method as an additional regularizer
- Interpretable geometric objective

#### Strategy 5: Selective Adaptation (Freeze Classifier Head)

**Concept**: Only adapt feature extractor, keep classifier head frozen after initial training.

```python
# Phase 1: Train classifier on source domain
model.train()
train_on_source(model, source_data)

# Phase 2: Freeze classifier, only adapt feature extractor
for param in model.class_classifier.parameters():
    param.requires_grad = False  # Freeze

# Only domain discriminator and feature extractor are trainable
domain_adapt(model, source_data, target_data)
```

**Benefits**:
- Guarantees classifier decision boundaries don't change
- Simple to implement
- Reduces risk of catastrophic forgetting

**Limitations**:
- May not fully adapt if classifier needs adjustment
- Assumes source classifier is optimal
- Less flexible than joint adaptation

#### Strategy 6: Multi-Task Learning with Auxiliary Tasks

**Concept**: Add auxiliary self-supervised tasks to maintain feature quality.

```python
# Auxiliary task: Rotation prediction (self-supervised)
def rotation_prediction_loss(features, images):
    # Rotate images by 0Â°, 90Â°, 180Â°, 270Â°
    rotated_images = apply_rotations(images)  # [4*B, ...]
    rotation_labels = torch.tensor([0, 1, 2, 3]).repeat(B)

    # Predict rotation angle
    rotation_logits = rotation_classifier(features)
    loss = F.cross_entropy(rotation_logits, rotation_labels)
    return loss

# Add to total loss
L_rotation = rotation_prediction_loss(features, images)
L_total = L_cls + Î»_adv*L_adv + Î»_mmd*L_mmd + Î»_rot*L_rotation
```

**Benefits**:
- Forces model to maintain rich, diverse features
- Self-supervised (no extra labels needed)
- Prevents feature collapse
- Used in Test-Time Training (TTT)

**Best Practice: Combined Monitoring Dashboard**

```python
class OverAlignmentMonitor:
    """Comprehensive monitoring for over-alignment"""

    def __init__(self, baseline_source_acc):
        self.baseline_source_acc = baseline_source_acc
        self.alerts = []

    def check_all(self, model, source_val_data, target_val_data, epoch):
        alerts = []

        # 1. Source accuracy
        source_acc = evaluate(model, source_val_data)
        if source_acc < self.baseline_source_acc * 0.95:
            alerts.append(f"âš ï¸ Source accuracy: {source_acc:.2%} (baseline: {self.baseline_source_acc:.2%})")

        # 2. Feature diversity
        features = extract_features(model, source_val_data)
        feature_std = torch.std(features, dim=0).mean().item()
        if feature_std < 0.1:
            alerts.append(f"âš ï¸ Low feature diversity: {feature_std:.4f}")

        # 3. Class separation
        separation_ratio = self.compute_class_separation(model, source_val_data)
        if separation_ratio < 2.0:
            alerts.append(f"âš ï¸ Poor class separation: {separation_ratio:.2f}")

        # 4. Orthogonality
        orth_loss = model.get_orthogonal_loss().item()
        if orth_loss > 0.5:
            alerts.append(f"âš ï¸ High orthogonality loss: {orth_loss:.4f}")

        # 5. Confusion matrix analysis
        conf_matrix = compute_confusion_matrix(model, source_val_data)
        off_diagonal_rate = (conf_matrix.sum() - conf_matrix.diag().sum()) / conf_matrix.sum()
        if off_diagonal_rate > 0.2:
            alerts.append(f"âš ï¸ High confusion rate: {off_diagonal_rate:.2%}")

        if alerts:
            print(f"\n{'='*60}")
            print(f"OVER-ALIGNMENT ALERTS - Epoch {epoch}")
            print(f"{'='*60}")
            for alert in alerts:
                print(alert)
            print(f"{'='*60}\n")

            # Recommend action
            if len(alerts) >= 3:
                print("ğŸ›‘ CRITICAL: Multiple over-alignment indicators!")
                print("   â†’ Increase Î»_orth (orthogonal constraint)")
                print("   â†’ Reduce Î»_adv and Î»_mmd")
                print("   â†’ Consider early stopping")

        return len(alerts) == 0  # True if healthy

# Usage in training loop
monitor = OverAlignmentMonitor(baseline_source_acc=0.85)

for epoch in range(num_epochs):
    train_one_epoch(...)

    # Check for over-alignment every 5 epochs
    if epoch % 5 == 0:
        is_healthy = monitor.check_all(model, source_val_data, target_val_data, epoch)

        if not is_healthy and epoch > 20:
            print("Stopping early due to over-alignment risk")
            break
```

**Recommended Approach** (Multi-Layer Defense):

1. **Primary**: Use **Orthogonal Constraint** (Strategy 1)
   - Add to all hybrid DA approaches
   - Start with `Î»_orth = 0.1`

2. **Secondary**: Implement **Monitoring** (Strategy 6)
   - Track all symptoms of over-alignment
   - Alert when metrics enter danger zone

3. **Backup**: Use **Progressive Adaptation** (Strategy 3)
   - Gradual increase in adaptation strength
   - Early stopping if source accuracy drops

4. **Optional**: Add **Class Separation Loss** (Strategy 4)
   - If domain gap is very large
   - If over-alignment still occurs with above strategies

**Expected Outcomes**:

| Scenario | Without Mitigation | With Orthogonal Constraint |
|----------|-------------------|---------------------------|
| Source accuracy | 85% â†’ 70% (drops) | 85% â†’ 83% (maintained) |
| Target accuracy | 40% â†’ 65% | 40% â†’ 72% (better!) |
| Domain gap | 25% â†’ 5% | 25% â†’ 11% |
| Class confusion | Low â†’ High | Low â†’ Low |
| Training stability | Unstable, diverges | Stable, converges |

**Key Takeaway**: Over-alignment is a **critical risk** when combining multiple domain adaptation methods. The **orthogonal constraint** is the most effective and efficient mitigation strategy, ensuring domain-invariant features while preserving class separability.

---

## 8. Evaluation & Monitoring

### 8.1 Experiment Tracking

**Use Weights & Biases (wandb)**:

```python
import wandb

# Initialize experiment
wandb.init(
    project="mine-site-domain-adaptation",
    config={
        "method": "DANN",
        "lambda_domain": 0.5,
        "source_site": "Site_A",
        "target_site": "Site_B",
    }
)

# Log metrics
wandb.log({
    "train/class_loss": class_loss,
    "train/domain_loss": domain_loss,
    "val/source_accuracy": source_acc,
    "val/target_accuracy": target_acc,
    "val/domain_gap": domain_gap,
})

# Log model
wandb.save("models/best_dann_model.pth")
```

**Track Key Metrics**:
- Training loss curves (classification + domain)
- Source/Target accuracy over epochs
- Domain gap over training
- Per-class F1 scores
- Feature distribution visualizations (t-SNE)
- Confusion matrices

### 8.2 A/B Testing Protocol

**Deployment Testing**:
```
1. Deploy baseline model (A) and adapted model (B) in parallel
2. Route 10% of samples to each for comparison
3. Collect predictions + ground truth (human verification)
4. Monitor for 1 week
5. Statistical significance test (paired t-test)
6. Full rollout if B significantly outperforms A
```

---

## 9. Documentation & Knowledge Transfer

### 9.1 Documentation Deliverables

1. **Technical Report** (`docs/domain_adaptation_report.pdf`):
   - Methods overview
   - Implementation details
   - Results and analysis
   - Recommendations

2. **API Documentation** (`docs/api_reference.md`):
   - Usage examples
   - Configuration options
   - Model selection guide

3. **Training Guide** (`docs/training_guide.md`):
   - Step-by-step training instructions
   - Hyperparameter tuning tips
   - Troubleshooting common issues

4. **Deployment Guide** (`docs/deployment_guide.md`):
   - Model export and optimization
   - Integration instructions
   - Monitoring and maintenance

### 9.2 Code Organization

```
domain-adapt/
â”œâ”€â”€ classification_model/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ bma_mil_model.py (existing)
â”‚   â”‚   â”‚   â”œâ”€â”€ domain_adversarial_mil.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ mmd_domain_adaptive_mil.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ contrastive_domain_adaptive_mil.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ multi_domain_mil.py (NEW)
â”‚   â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”‚   â”œâ”€â”€ mmd_loss.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ contrastive_domain_loss.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ adversarial_loss.py (NEW)
â”‚   â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”‚   â”œâ”€â”€ domain_adversarial_training.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ mmd_training.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ self_training.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ multi_domain_training.py (NEW)
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”œâ”€â”€ multi_domain_dataset.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ multi_domain_loader.py (NEW)
â”‚   â”‚   â”œâ”€â”€ augmentation/
â”‚   â”‚   â”‚   â”œâ”€â”€ mining_augmentation.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ color_transformations.py (NEW)
â”‚   â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”‚   â”œâ”€â”€ domain_evaluation.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ cross_domain_metrics.py (NEW)
â”‚   â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_time_adaptation.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ ensemble_inference.py (NEW)
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ gradient_reversal.py (NEW)
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ train_domain_adaptation.py (NEW)
â”‚   â”‚   â”œâ”€â”€ evaluate_cross_domain.py (NEW)
â”‚   â”‚   â””â”€â”€ compare_methods.py (NEW)
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ domain_adaptation_config.py (NEW)
â”‚   â””â”€â”€ notebooks/
â”‚       â”œâ”€â”€ domain_analysis.ipynb (NEW)
â”‚       â””â”€â”€ method_comparison.ipynb (NEW)
â””â”€â”€ docs/
    â”œâ”€â”€ domain_adaptation_report.pdf (NEW)
    â”œâ”€â”€ training_guide.md (NEW)
    â””â”€â”€ api_reference.md (NEW)
```

---

## 10. Future Directions

### 10.1 Advanced Research Directions

1. **Continual Learning**:
   - Learn from streaming data across multiple sites
   - Avoid catastrophic forgetting
   - Incremental class learning

2. **Open-Set Domain Adaptation**:
   - Detect novel spoil classes not in training data
   - "Unknown" class modeling
   - Out-of-distribution detection

3. **Explainable Domain Adaptation**:
   - Visualize what features transfer across domains
   - Identify domain-specific vs. domain-invariant features
   - Generate explanations for predictions

4. **Federated Learning**:
   - Train collaboratively across multiple mine sites
   - Preserve data privacy
   - Distributed domain adaptation

5. **Self-Supervised Pre-Training**:
   - Pre-train on unlabeled mining imagery
   - Learn mining-specific representations
   - Reduce reliance on ImageNet features

### 10.2 Practical Extensions

1. **Active Learning Integration**:
   - Identify most informative samples for labeling
   - Query strategy based on domain confusion
   - Budget-constrained labeling

2. **Multi-Modal Fusion**:
   - Incorporate metadata (GPS, time, camera model)
   - Geological survey data
   - Sensor data (moisture, composition)

3. **Temporal Adaptation**:
   - Handle seasonal changes at same mine site
   - Weather condition adaptation
   - Time-of-day invariance

4. **Mobile Deployment**:
   - Model compression for edge devices
   - Quantization and pruning
   - On-device adaptation

---

## 11. Conclusion & Recommendations

### 11.1 Recommended Approach

**Phase 1** (Immediate - Weeks 1-6):
1. âœ… Implement **Adversarial Domain Adaptation (DANN)**
   - Most mature and widely-used method
   - Strong theoretical foundation
   - Good balance of performance and complexity

2. âœ… Implement **MMD-based Adaptation**
   - Simpler than adversarial approach
   - No instability issues
   - Effective for covariate shift

3. âœ… Enhance **Data Augmentation**
   - Mining-specific color transformations
   - Illumination simulation
   - Fast wins with existing infrastructure

**Phase 2** (Medium-term - Weeks 7-11):
4. âš¡ **Self-Training Pipeline**
   - Leverage unlabeled target data
   - Progressive confidence thresholding
   - High impact for unlabeled scenarios

5. âš¡ **Multi-Source Learning**
   - If multiple mine sites available
   - Learn domain-agnostic features
   - Best long-term generalization

**Phase 3** (Advanced - Weeks 12-13):
6. ğŸ”¬ **Test-Time Adaptation**
   - Deploy-time flexibility
   - Continuous adaptation
   - Hedge against distribution shift

### 11.2 Key Success Factors

1. **Data Quality**: Ensure target domain data is representative
2. **Hyperparameter Tuning**: Careful tuning of Î»_domain, confidence thresholds
3. **Validation Strategy**: Hold-out target validation set is critical
4. **Iterative Development**: Start simple, add complexity gradually
5. **Monitoring**: Continuous performance tracking in production

### 11.3 Expected Impact

**Technical Impact**:
- **+20-30% accuracy improvement** on new mine sites
- **<10% domain gap** (from ~35%)
- **Reduced data labeling requirements** by 80%

**Business Impact**:
- Deploy to new mine sites with minimal retraining
- Faster rollout to new customers
- Reduced operational costs (less manual labeling)
- Improved safety and efficiency in mining operations

---

## References

### Key Papers

1. **Adversarial Domain Adaptation**:
   - Ganin et al. (2016). "Domain-Adversarial Training of Neural Networks". JMLR.
   - Tzeng et al. (2017). "Adversarial Discriminative Domain Adaptation". CVPR.

2. **MMD-Based Methods**:
   - Long et al. (2015). "Learning Transferable Features with Deep Adaptation Networks". ICML.
   - Long et al. (2017). "Conditional Adversarial Domain Adaptation". NeurIPS.

3. **Self-Training**:
   - Xie et al. (2020). "Self-training with Noisy Student improves ImageNet classification". CVPR.
   - Sohn et al. (2020). "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence". NeurIPS.

4. **Contrastive Learning**:
   - Khosla et al. (2020). "Supervised Contrastive Learning". NeurIPS.
   - Saito et al. (2021). "Cross-Domain Few-Shot Learning with Task-Specific Adapters". CVPR.

5. **Test-Time Adaptation**:
   - Wang et al. (2021). "Tent: Fully Test-Time Adaptation by Entropy Minimization". ICLR.
   - Sun et al. (2020). "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts". ICML.

6. **Multi-Source Domain Adaptation**:
   - Peng et al. (2019). "Moment Matching for Multi-Source Domain Adaptation". ICCV.
   - Zhao et al. (2020). "Multi-Source Distilling Domain Adaptation". AAAI.

### Domain Adaptation Surveys

- Wang & Deng (2018). "Deep Visual Domain Adaptation: A Survey". Neurocomputing.
- Wilson & Cook (2020). "A Survey of Unsupervised Deep Domain Adaptation". ACM TIST.
- Csurka (2021). "Domain Adaptation for Visual Applications: A Comprehensive Survey". Springer.

---

**Document Version**: 1.0
**Last Updated**: 2025-11-05
**Author**: Claude (AI Assistant)
**Status**: Ready for Implementation
