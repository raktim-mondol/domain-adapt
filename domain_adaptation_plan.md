# Comprehensive Domain Adaptation Plan for Mine Site Spoil Classification

## Executive Summary

This document outlines a comprehensive strategy to adapt the current BMA MIL (Multiple Instance Learning) spoil classification model to generalize across different mine sites. The model currently achieves 83-88% accuracy on the source mine site but shows degraded performance on new mine sites due to domain shift.

**Key Challenge**: Cross-mine site domain adaptation where visual characteristics (lighting, soil composition, camera angles, weathering patterns) vary significantly between locations.

**Solution Approach**: Multi-stage domain adaptation pipeline combining state-of-the-art techniques specifically designed for vision-based classification with limited target domain labels.

---

## 1. Problem Analysis

### 1.1 Domain Shift Characteristics in Mine Sites

**Source Domain** (Training Site):
- Specific geological composition and color profiles
- Consistent lighting conditions and camera setup
- Particular weathering and oxidation patterns
- Site-specific equipment and capture protocols

**Target Domains** (New Mine Sites):
- Different mineral compositions â†’ color shifts
- Varying illumination (time of day, weather, seasons)
- Different camera models and calibration
- Unique geological stratification patterns
- Site-specific environmental conditions

### 1.2 Types of Domain Shift

1. **Covariate Shift**: P_source(X) â‰  P_target(X)
   - Input distribution changes (color, texture, lighting)
   - Label distribution remains similar

2. **Label Shift**: P_source(Y) â‰  P_target(Y)
   - Different proportions of spoil classes
   - Some classes may be rare/absent at certain sites

3. **Concept Shift**: P_source(Y|X) â‰  P_target(Y|X)
   - Same visual appearance may indicate different classes
   - Geological context affects interpretation

### 1.3 Current System Limitations

| Component | Current Approach | Limitation |
|-----------|------------------|------------|
| Feature Extraction | ImageNet pre-trained ViT-R50 | Not adapted to mining domain |
| Training | Single-source supervised | No cross-domain alignment |
| Augmentation | Generic geometric transforms | Doesn't model domain shifts |
| Evaluation | Single-domain validation | No generalization metrics |
| Architecture | Standard MIL classifier | No domain-invariant learning |

---

## 2. State-of-the-Art Domain Adaptation Techniques

### 2.1 Deep Domain Adaptation Methods

#### 2.1.1 **Adversarial Domain Adaptation (ADA)**

**Concept**: Learn domain-invariant features by fooling a domain discriminator.

**Key Papers**:
- DANN (Domain-Adversarial Neural Networks) - Ganin et al., 2016
- ADDA (Adversarial Discriminative Domain Adaptation) - Tzeng et al., 2017
- CDAN (Conditional Domain Adversarial Network) - Long et al., 2018

**Architecture**:
```
Input Image â†’ Feature Extractor (ViT-R50) â†’ Features
                                                â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â†“                                           â†“
              Class Classifier                          Domain Discriminator
                  (3 classes)                          (Source vs Target)
                        â†“                                           â†“
              Classification Loss                 Adversarial Loss (GRL)
```

**Advantages**:
- Strong theoretical foundation
- Learns domain-invariant representations
- Proven effectiveness on visual tasks

**Considerations for MIL**:
- Apply adversarial loss at both patch and bag levels
- Use Gradient Reversal Layer (GRL) for stable training

---

#### 2.1.2 **Maximum Mean Discrepancy (MMD)**

**Concept**: Minimize statistical distance between source and target feature distributions.

**Key Papers**:
- Deep Domain Confusion (DDC) - Tzeng et al., 2014
- Deep Adaptation Networks (DAN) - Long et al., 2015
- Joint Adaptation Networks (JAN) - Long et al., 2017

**Loss Function**:
```
MMD(Xs, Xt) = ||1/ns Î£Ï†(xs_i) - 1/nt Î£Ï†(xt_j)||Â²
```

**Multi-Kernel MMD**:
```
MMD_k = Î£_k Î²_k * MMD(Xs, Xt; kernel_k)
```

**Advantages**:
- No additional discriminator needed
- Explicit distribution matching
- Effective for covariate shift

**Implementation Strategy**:
- Apply MMD at multiple feature levels (patch, bag, pile)
- Use multiple kernels (Gaussian with different bandwidths)

---

#### 2.1.3 **Self-Training and Pseudo-Labeling**

**Concept**: Use confident predictions on target domain as pseudo-labels for retraining.

**Key Papers**:
- Noisy Student - Xie et al., 2020
- Meta Pseudo Labels - Pham et al., 2021
- FixMatch - Sohn et al., 2020

**Pipeline**:
```
1. Train on source domain
2. Generate pseudo-labels for target domain (high confidence only)
3. Retrain with source labels + target pseudo-labels
4. Iterate with confidence threshold scheduling
```

**Advanced Variant - Teacher-Student Framework**:
- Teacher model: Generates pseudo-labels
- Student model: Trained on pseudo-labels + strong augmentation
- Progressive self-training with EMA updates

**Advantages**:
- No target domain labels required
- Simple to implement
- Effective when model confidence is reliable

---

#### 2.1.4 **Contrastive Domain Adaptation**

**Concept**: Learn representations where same-class samples cluster together across domains.

**Key Papers**:
- SupCon (Supervised Contrastive Learning) - Khosla et al., 2020
- CrossMatch - Saito et al., 2021
- CD3A (Contrastive Domain Discrepancy for Domain Adaptation) - Zhao et al., 2021

**Loss Function**:
```
L_contrastive = -log(exp(sim(z_i, z_i+)/Ï„) / Î£_j exp(sim(z_i, z_j)/Ï„))

Where:
- z_i: anchor sample features
- z_i+: positive samples (same class, any domain)
- z_j: all samples in batch
- Ï„: temperature parameter
```

**Domain-Aware Contrastive Loss**:
```
L_DAC = L_intra-class + L_inter-domain + L_inter-class
```

**Advantages**:
- Learns discriminative features
- Explicitly reduces domain gap
- Effective with limited target labels

---

#### 2.1.5 **Meta-Learning for Domain Adaptation**

**Concept**: Learn to quickly adapt to new domains with few samples.

**Key Papers**:
- MAML (Model-Agnostic Meta-Learning) - Finn et al., 2017
- Meta-DAN - Ma et al., 2019
- DLOW (Domain-specific Learning with Optimization-based Weights) - Gao et al., 2020

**Training Strategy**:
```
Meta-Training Phase:
- Simulate domain shift using data augmentation
- Learn initialization that adapts quickly
- Optimize for few-shot adaptation

Meta-Testing Phase:
- Fine-tune on few labeled target samples
- Fast adaptation to new mine sites
```

**Advantages**:
- Few-shot learning capability
- Generalizes to unseen domains
- Suitable for limited target data scenarios

---

### 2.2 Domain-Specific Augmentation Techniques

#### 2.2.1 **Style Transfer for Domain Bridging**

**Concept**: Transform source images to mimic target domain appearance.

**Key Techniques**:
- **AdaIN (Adaptive Instance Normalization)** - Huang et al., 2017
- **CycleGAN** - Zhu et al., 2017
- **Neural Style Transfer**

**Application to Mine Sites**:
```
Source Site Images â†’ Style Transfer â†’ "Target-style" Images
                                              â†“
                                     Train with both original
                                     and style-transferred images
```

**Mining-Specific Augmentations**:
- Color shift simulation (mineral composition variations)
- Illumination changes (time-of-day, weather)
- Texture synthesis (weathering patterns)
- Camera response simulation

---

#### 2.2.2 **Adversarial Data Augmentation**

**Concept**: Generate augmentations that maximize domain confusion.

**Implementation**:
```python
# Learn domain-specific augmentation parameters
aug_params = AugmentationGenerator(domain_classifier_loss)
augmented_images = apply_augmentation(images, aug_params)
```

**Benefits**:
- Data-driven augmentation strategy
- Specifically targets domain gap
- No need for target domain data during augmentation design

---

### 2.3 Test-Time Adaptation Techniques

#### 2.3.1 **Test-Time Training (TTT)**

**Concept**: Continue adapting model during inference on target domain.

**Key Papers**:
- TTT - Sun et al., 2020
- TENT (Test Entropy Minimization) - Wang et al., 2021
- NOTE (Normalization for Test-Time Adaptation) - Gong et al., 2022

**Strategy**:
```
At inference time on new mine site:
1. Collect unlabeled test samples
2. Adapt batch normalization statistics
3. Minimize entropy of predictions
4. Update model parameters
```

**Advantages**:
- No target labels needed
- Adapts to test distribution
- Can be combined with other methods

---

#### 2.3.2 **Online Domain Adaptation**

**Concept**: Incrementally adapt as new samples arrive.

**Implementation**:
```
Initial model (trained on source)
    â†“
Deploy to new mine site
    â†“
Collect predictions + optional human feedback
    â†“
Continuously update model (online learning)
```

---

### 2.4 Multi-Source Domain Adaptation

#### 2.4.1 **Multiple Mine Sites as Sources**

**Concept**: Train on multiple mine sites to learn domain-agnostic features.

**Key Papers**:
- DomainNet - Peng et al., 2019
- MDDA (Multi-Domain Domain Adaptation) - Zhao et al., 2020

**Training Strategy**:
```
Mine Site A (labeled) â”€â”€â”
Mine Site B (labeled) â”€â”€â”¼â”€â”€â†’ Multi-source training â†’ Generalized model
Mine Site C (labeled) â”€â”€â”˜
                                    â†“
                            Deploy to Mine Site D (unlabeled)
```

**Domain Aggregation Methods**:
- Domain-specific batch normalization layers
- Attention-based domain weighting
- Meta-learning across sources

---

### 2.5 Unsupervised Domain Adaptation (UDA)

**Assumption**: Target domain has no labels.

**Key Methods**:

1. **Source-Free Domain Adaptation**
   - Adapt using only source model + target data
   - Protect source data privacy
   - Information maximization on target domain

2. **Universal Domain Adaptation**
   - Handle label set mismatch (target may have different classes)
   - "Unknown" class detection
   - Important if new mine sites have unique spoil types

---

## 3. Recommended Implementation Strategy

### 3.1 Phase 1: Foundation (Weeks 1-2)

#### 3.1.1 **Data Infrastructure**

**Dataset Organization**:
```
data/
â”œâ”€â”€ source_site/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ target_site_1/
â”‚   â”œâ”€â”€ unlabeled/
â”‚   â””â”€â”€ few_shot_labeled/ (optional)
â”œâ”€â”€ target_site_2/
â”‚   â””â”€â”€ unlabeled/
â””â”€â”€ metadata.json (domain labels, site characteristics)
```

**Implementation Tasks**:
- [ ] Create multi-domain dataset loader
- [ ] Add domain labels to CSV (site_id column)
- [ ] Implement domain-stratified data splitting
- [ ] Create visualization tools for domain statistics

**File**: `src/data/multi_domain_dataset.py`

---

#### 3.1.2 **Baseline Evaluation**

**Zero-Shot Transfer Evaluation**:
```python
# Train on source mine site
model.train(source_data)

# Evaluate on target mine sites (no adaptation)
for target_site in target_sites:
    metrics = model.evaluate(target_site)
    log_performance_drop(source_metrics, target_metrics)
```

**Metrics to Track**:
- Source domain accuracy (baseline)
- Target domain accuracy (zero-shot)
- Domain gap (performance difference)
- Per-class performance degradation
- Confusion matrix comparison

**Deliverable**: `results/baseline_domain_gap_analysis.pdf`

---

### 3.2 Phase 2: Core Domain Adaptation (Weeks 3-6)

#### 3.2.1 **Method 1: Adversarial Domain Adaptation (Priority 1)**

**Architecture Design**:

```python
class DomainAdaptiveMIL(nn.Module):
    def __init__(self):
        # Shared feature extractor
        self.feature_extractor = ViTR50()  # ViT-R50 backbone

        # MIL attention aggregator
        self.attention_aggregator = AttentionAggregator()

        # Task classifier (3 classes)
        self.class_classifier = ClassClassifier()

        # Domain discriminator (source vs target)
        self.domain_discriminator = DomainDiscriminator()

        # Gradient reversal layer
        self.grl = GradientReversalLayer(lambda_=1.0)

    def forward(self, bags, alpha):
        # Extract features
        patch_features = self.feature_extractor(bags)  # [B, N, 768]

        # Aggregate with attention
        bag_features, attention = self.attention_aggregator(patch_features)

        # Classification
        class_logits = self.class_classifier(bag_features)

        # Domain classification with gradient reversal
        domain_features = self.grl(bag_features, alpha)
        domain_logits = self.domain_discriminator(domain_features)

        return class_logits, domain_logits, attention
```

**Training Loop**:
```python
for epoch in range(num_epochs):
    for source_bags, target_bags in zip(source_loader, target_loader):
        # Forward pass
        source_class_logits, source_domain_logits, _ = model(source_bags, alpha)
        target_class_logits, target_domain_logits, _ = model(target_bags, alpha)

        # Classification loss (source only)
        class_loss = criterion(source_class_logits, source_labels)

        # Domain confusion loss
        source_domain_labels = torch.zeros(len(source_bags))
        target_domain_labels = torch.ones(len(target_bags))
        domain_loss = criterion(source_domain_logits, source_domain_labels) + \
                      criterion(target_domain_logits, target_domain_labels)

        # Total loss
        total_loss = class_loss + lambda_domain * domain_loss

        # Backpropagation
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        # Update lambda (GRL weight) - increase over time
        alpha = 2 / (1 + np.exp(-10 * epoch / num_epochs)) - 1
```

**Hyperparameters**:
- `lambda_domain`: 0.1 â†’ 1.0 (scheduled increase)
- `alpha`: 0.0 â†’ 1.0 (GRL strength, scheduled)
- Domain discriminator architecture: 512â†’256â†’128â†’1
- Learning rate: 1e-4 (feature extractor), 1e-3 (discriminator)

**Implementation Files**:
- `src/models/domain_adversarial_mil.py`
- `src/utils/gradient_reversal.py`
- `src/utils/domain_adversarial_training.py`

---

#### 3.2.2 **Method 2: MMD-Based Adaptation (Priority 1)**

**Loss Function**:

```python
def mmd_loss(source_features, target_features, kernel_mul=2.0, kernel_num=5):
    """
    Multi-kernel Maximum Mean Discrepancy
    """
    batch_size = source_features.size(0)

    # Compute kernels
    kernels = []
    for i in range(kernel_num):
        bandwidth = kernel_mul ** i
        kernels.append(gaussian_kernel(source_features, target_features, bandwidth))

    # Average over kernels
    mmd = sum(kernels) / len(kernels)

    return mmd

def gaussian_kernel(x, y, bandwidth):
    """Gaussian RBF kernel"""
    n_x = x.size(0)
    n_y = y.size(0)

    # Pairwise distances
    xx = torch.mm(x, x.t())
    yy = torch.mm(y, y.t())
    xy = torch.mm(x, y.t())

    # Compute kernel
    X2 = xx.diag().unsqueeze(0).expand_as(xx)
    Y2 = yy.diag().unsqueeze(0).expand_as(yy)

    K_xx = torch.exp(-bandwidth * (X2 + X2.t() - 2 * xx))
    K_yy = torch.exp(-bandwidth * (Y2 + Y2.t() - 2 * yy))
    K_xy = torch.exp(-bandwidth * (X2 + Y2.t() - 2 * xy))

    return K_xx.mean() + K_yy.mean() - 2 * K_xy.mean()
```

**Multi-Level MMD**:
```python
# Apply MMD at multiple levels
mmd_patch = mmd_loss(source_patch_features, target_patch_features)
mmd_bag = mmd_loss(source_bag_features, target_bag_features)
mmd_pile = mmd_loss(source_pile_features, target_pile_features)

total_mmd = alpha_patch * mmd_patch + alpha_bag * mmd_bag + alpha_pile * mmd_pile
total_loss = classification_loss + lambda_mmd * total_mmd
```

**Advantages for MIL**:
- Aligns distributions at all hierarchy levels
- No additional network components
- Stable training (no adversarial dynamics)

**Implementation Files**:
- `src/losses/mmd_loss.py`
- `src/models/mmd_domain_adaptive_mil.py`
- `src/utils/mmd_training.py`

---

#### 3.2.3 **Method 3: Contrastive Domain Adaptation (Priority 2)**

**Contrastive Loss Design**:

```python
class ContrastiveDomainLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, features, labels, domains):
        """
        features: [B, D] normalized features
        labels: [B] class labels
        domains: [B] domain labels (0=source, 1=target)
        """
        # Compute similarity matrix
        sim_matrix = torch.matmul(features, features.T) / self.temperature

        # Create positive mask (same class, different domain preferred)
        label_mask = labels.unsqueeze(0) == labels.unsqueeze(1)
        domain_mask = domains.unsqueeze(0) != domains.unsqueeze(1)
        positive_mask = label_mask & domain_mask  # Same class, different domain

        # Create negative mask (different class)
        negative_mask = ~label_mask

        # Compute loss
        exp_sim = torch.exp(sim_matrix)

        # Positive pairs: same class across domains
        pos_sim = (exp_sim * positive_mask).sum(1)

        # Negative pairs: different class
        neg_sim = (exp_sim * negative_mask).sum(1)

        # Contrastive loss
        loss = -torch.log(pos_sim / (pos_sim + neg_sim + 1e-8))

        return loss.mean()
```

**Training Strategy**:
```python
# Projection head for contrastive learning
projection_head = nn.Sequential(
    nn.Linear(768, 512),
    nn.ReLU(),
    nn.Linear(512, 256)
)

# Training loop
for source_bags, target_bags in loader:
    # Extract features
    source_features = model.feature_extractor(source_bags)
    target_features = model.feature_extractor(target_bags)

    # Project for contrastive learning
    source_proj = projection_head(source_features)
    target_proj = projection_head(target_features)

    # Combine
    all_features = torch.cat([source_proj, target_proj])
    all_labels = torch.cat([source_labels, pseudo_target_labels])
    all_domains = torch.cat([
        torch.zeros(len(source_bags)),
        torch.ones(len(target_bags))
    ])

    # Contrastive loss
    contrastive_loss = criterion(all_features, all_labels, all_domains)

    # Classification loss
    class_loss = ce_loss(source_class_logits, source_labels)

    # Total loss
    total_loss = class_loss + lambda_contrast * contrastive_loss
```

**Benefits**:
- Learns discriminative cross-domain features
- Reduces intra-class domain gap
- Maintains inter-class separation

**Implementation Files**:
- `src/losses/contrastive_domain_loss.py`
- `src/models/contrastive_domain_adaptive_mil.py`

---

### 3.3 Phase 3: Advanced Techniques (Weeks 7-9)

#### 3.3.1 **Self-Training with Pseudo-Labels**

**Pipeline**:

```python
class SelfTrainingPipeline:
    def __init__(self, base_model, confidence_threshold=0.9):
        self.teacher_model = base_model
        self.student_model = copy.deepcopy(base_model)
        self.confidence_threshold = confidence_threshold

    def train(self, source_data, target_data, num_iterations=5):
        for iteration in range(num_iterations):
            print(f"Self-training iteration {iteration+1}")

            # 1. Generate pseudo-labels on target domain
            pseudo_labels, confidences = self.generate_pseudo_labels(target_data)

            # 2. Select high-confidence samples
            high_conf_mask = confidences > self.confidence_threshold
            selected_target_data = target_data[high_conf_mask]
            selected_pseudo_labels = pseudo_labels[high_conf_mask]

            # 3. Combine source and pseudo-labeled target data
            combined_data = source_data + (selected_target_data, selected_pseudo_labels)

            # 4. Train student model
            self.student_model.train(combined_data)

            # 5. Update teacher with EMA
            self.teacher_model = self.ema_update(self.teacher_model, self.student_model)

            # 6. Lower confidence threshold (progressive)
            self.confidence_threshold *= 0.95

    def generate_pseudo_labels(self, target_data):
        """Generate pseudo-labels using teacher model"""
        self.teacher_model.eval()
        with torch.no_grad():
            logits = self.teacher_model(target_data)
            probs = F.softmax(logits, dim=1)
            confidences, pseudo_labels = probs.max(dim=1)
        return pseudo_labels, confidences

    def ema_update(self, teacher, student, alpha=0.999):
        """Exponential moving average update"""
        for teacher_param, student_param in zip(teacher.parameters(), student.parameters()):
            teacher_param.data = alpha * teacher_param.data + (1 - alpha) * student_param.data
        return teacher
```

**Progressive Thresholding Schedule**:
```
Iteration 1: threshold = 0.95 (very high confidence only)
Iteration 2: threshold = 0.90
Iteration 3: threshold = 0.85
Iteration 4: threshold = 0.80
Iteration 5: threshold = 0.75
```

**Class-Balanced Sampling**:
```python
# Ensure balanced pseudo-labeled samples per class
def balanced_pseudo_label_sampling(pseudo_labels, confidences, samples_per_class=100):
    selected_indices = []
    for class_id in range(num_classes):
        class_mask = pseudo_labels == class_id
        class_confidences = confidences[class_mask]

        # Select top-k confident samples
        top_k_indices = torch.topk(class_confidences, k=samples_per_class).indices
        selected_indices.extend(top_k_indices)

    return selected_indices
```

**Implementation Files**:
- `src/training/self_training.py`
- `src/training/pseudo_labeling.py`

---

#### 3.3.2 **Mining-Specific Augmentation**

**Color Space Transformations**:

```python
class MiningColorAugmentation:
    """Simulate color variations across mine sites"""

    def __init__(self):
        # Different mineral compositions â†’ different color profiles
        self.color_shifts = {
            'iron_rich': {'r': 1.2, 'g': 0.9, 'b': 0.8},      # Reddish
            'clay_rich': {'r': 1.1, 'g': 1.1, 'b': 0.9},       # Yellowish
            'limestone': {'r': 1.05, 'g': 1.05, 'b': 1.05},    # Whitish
            'coal': {'r': 0.8, 'g': 0.8, 'b': 0.8},            # Darker
        }

    def apply(self, image, mineral_type=None):
        """Apply mineral-specific color transformation"""
        if mineral_type is None:
            mineral_type = random.choice(list(self.color_shifts.keys()))

        shifts = self.color_shifts[mineral_type]

        # Apply to RGB channels
        image[:, :, 0] *= shifts['r']
        image[:, :, 1] *= shifts['g']
        image[:, :, 2] *= shifts['b']

        return torch.clamp(image, 0, 1)

class IlluminationAugmentation:
    """Simulate different lighting conditions"""

    def apply(self, image):
        # Simulate different times of day
        brightness_factor = random.uniform(0.6, 1.4)

        # Simulate shadow patterns (localized darkening)
        shadow_mask = self.generate_shadow_mask(image.shape)

        augmented = image * brightness_factor * shadow_mask
        return torch.clamp(augmented, 0, 1)

    def generate_shadow_mask(self, shape):
        """Generate realistic shadow patterns"""
        # Use Perlin noise for natural-looking shadows
        mask = perlin_noise(shape, scale=50)
        mask = (mask + 1) / 2  # Normalize to [0, 1]
        mask = mask * 0.5 + 0.5  # Range [0.5, 1.0]
        return mask

class WeatheringAugmentation:
    """Simulate weathering and oxidation patterns"""

    def apply(self, image):
        # Add texture noise (weathering patterns)
        noise = torch.randn_like(image) * 0.05

        # Apply spatially-varying oxidation (reddish tint in patches)
        oxidation_mask = self.generate_oxidation_mask(image.shape)
        oxidation_tint = torch.tensor([1.3, 0.9, 0.7])  # Reddish

        augmented = image + noise
        augmented = augmented * (1 - oxidation_mask) + \
                    (image * oxidation_tint) * oxidation_mask

        return torch.clamp(augmented, 0, 1)
```

**Integration into Training**:
```python
augmentation_pipeline = Compose([
    MiningColorAugmentation(),
    IlluminationAugmentation(),
    WeatheringAugmentation(),
    RandomRotation(15),
    RandomZoom(0.9, 2.5),
    RandomHorizontalFlip(),
])
```

**Implementation Files**:
- `src/augmentation/mining_augmentation.py`
- `src/augmentation/color_transformations.py`

---

#### 3.3.3 **Test-Time Adaptation**

**Entropy Minimization**:

```python
class TestTimeAdaptation:
    def __init__(self, model):
        self.model = model
        self.adapted_model = copy.deepcopy(model)

        # Only adapt batch normalization and attention layers
        self.configure_trainable_params()

    def configure_trainable_params(self):
        """Freeze all except BN and attention"""
        for param in self.adapted_model.parameters():
            param.requires_grad = False

        # Enable BN layers
        for module in self.adapted_model.modules():
            if isinstance(module, nn.BatchNorm2d):
                module.requires_grad = True
                module.track_running_stats = False  # Use batch statistics

        # Enable attention aggregator
        for param in self.adapted_model.attention_aggregator.parameters():
            param.requires_grad = True

    def adapt(self, test_batch, num_steps=5, lr=1e-3):
        """Adapt model to test batch"""
        optimizer = torch.optim.Adam(
            filter(lambda p: p.requires_grad, self.adapted_model.parameters()),
            lr=lr
        )

        for step in range(num_steps):
            # Forward pass
            logits = self.adapted_model(test_batch)
            probs = F.softmax(logits, dim=1)

            # Entropy minimization loss
            entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1).mean()

            # Diversity regularization (prevent collapse)
            avg_probs = probs.mean(dim=0)
            diversity = (avg_probs * torch.log(avg_probs + 1e-8)).sum()

            # Total loss
            loss = entropy + 0.1 * diversity

            # Update
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        return self.adapted_model

    def predict(self, test_data, adapt_batch_size=32):
        """Predict with test-time adaptation"""
        predictions = []

        # Process in batches
        for batch in DataLoader(test_data, batch_size=adapt_batch_size):
            # Adapt to this batch
            adapted_model = self.adapt(batch)

            # Predict
            with torch.no_grad():
                logits = adapted_model(batch)
                preds = logits.argmax(dim=1)

            predictions.extend(preds)

        return predictions
```

**Implementation Files**:
- `src/inference/test_time_adaptation.py`

---

### 3.4 Phase 4: Multi-Source Learning (Weeks 10-11)

#### 3.4.1 **Multi-Domain Dataset Preparation**

**Data Collection Strategy**:
```
Collect data from â‰¥3 diverse mine sites:
- Site A: Iron-rich geology
- Site B: Clay-rich geology
- Site C: Limestone-dominant
- Site D: Mixed geology (target for testing)
```

**Domain-Specific Batch Normalization**:

```python
class DomainSpecificBatchNorm(nn.Module):
    """Separate BN statistics for each domain"""

    def __init__(self, num_features, num_domains):
        super().__init__()
        self.num_domains = num_domains

        # Create separate BN layers for each domain
        self.bn_layers = nn.ModuleList([
            nn.BatchNorm2d(num_features) for _ in range(num_domains)
        ])

    def forward(self, x, domain_id):
        """
        x: input features [B, C, H, W]
        domain_id: domain identifier [B]
        """
        # Apply domain-specific BN
        output = torch.zeros_like(x)
        for domain in range(self.num_domains):
            mask = domain_id == domain
            if mask.any():
                output[mask] = self.bn_layers[domain](x[mask])

        return output
```

**Domain Attention Network**:

```python
class DomainAttentionAggregator(nn.Module):
    """Learn to weight different source domains"""

    def __init__(self, feature_dim, num_domains):
        super().__init__()
        self.domain_attention = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_domains),
            nn.Softmax(dim=1)
        )

    def forward(self, features, domain_features):
        """
        features: [B, D] input features
        domain_features: [num_domains, D] domain-specific features
        """
        # Compute attention weights over domains
        attention_weights = self.domain_attention(features)  # [B, num_domains]

        # Weighted combination of domain-specific features
        weighted_features = torch.einsum('bd,df->bf', attention_weights, domain_features)

        return weighted_features, attention_weights
```

**Implementation Files**:
- `src/models/multi_domain_mil.py`
- `src/data/multi_domain_loader.py`

---

### 3.5 Phase 5: Evaluation & Deployment (Weeks 12-13)

#### 3.5.1 **Comprehensive Evaluation Protocol**

**Cross-Domain Evaluation Metrics**:

```python
class DomainAdaptationEvaluator:
    """Comprehensive evaluation for domain adaptation"""

    def evaluate(self, model, source_data, target_data):
        metrics = {}

        # 1. Source domain performance (should remain high)
        metrics['source_accuracy'] = self.compute_accuracy(model, source_data)

        # 2. Target domain performance (primary goal)
        metrics['target_accuracy'] = self.compute_accuracy(model, target_data)

        # 3. Domain gap (reduction)
        metrics['domain_gap'] = metrics['source_accuracy'] - metrics['target_accuracy']

        # 4. Class-wise performance
        metrics['source_per_class_f1'] = self.compute_per_class_f1(model, source_data)
        metrics['target_per_class_f1'] = self.compute_per_class_f1(model, target_data)

        # 5. Feature distribution alignment
        metrics['mmd_distance'] = self.compute_mmd(
            model.extract_features(source_data),
            model.extract_features(target_data)
        )

        # 6. Domain confusion (ideal: 50% for perfect alignment)
        metrics['domain_confusion'] = self.compute_domain_classification_accuracy(
            model.extract_features(source_data),
            model.extract_features(target_data)
        )

        # 7. Calibration error
        metrics['target_ece'] = self.expected_calibration_error(model, target_data)

        return metrics

    def expected_calibration_error(self, model, data, num_bins=10):
        """Measure prediction confidence calibration"""
        predictions, confidences, true_labels = [], [], []

        with torch.no_grad():
            for batch, labels in data:
                logits = model(batch)
                probs = F.softmax(logits, dim=1)
                conf, pred = probs.max(dim=1)

                predictions.extend(pred.cpu().numpy())
                confidences.extend(conf.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())

        # Bin predictions by confidence
        ece = 0
        for bin_idx in range(num_bins):
            bin_lower = bin_idx / num_bins
            bin_upper = (bin_idx + 1) / num_bins

            in_bin = [(bin_lower <= c < bin_upper) for c in confidences]
            if sum(in_bin) == 0:
                continue

            bin_accuracy = np.mean([p == l for p, l, ib in
                                   zip(predictions, true_labels, in_bin) if ib])
            bin_confidence = np.mean([c for c, ib in
                                     zip(confidences, in_bin) if ib])

            ece += (bin_accuracy - bin_confidence) ** 2 * sum(in_bin) / len(predictions)

        return ece
```

**Visualization Tools**:

```python
def visualize_domain_adaptation(source_features, target_features, method='tsne'):
    """Visualize feature distributions before/after adaptation"""
    from sklearn.manifold import TSNE
    import matplotlib.pyplot as plt

    # Reduce dimensions
    if method == 'tsne':
        embeddings = TSNE(n_components=2).fit_transform(
            np.vstack([source_features, target_features])
        )

    # Plot
    plt.figure(figsize=(10, 5))

    # Source domain
    plt.scatter(embeddings[:len(source_features), 0],
               embeddings[:len(source_features), 1],
               c='blue', label='Source', alpha=0.5)

    # Target domain
    plt.scatter(embeddings[len(source_features):, 0],
               embeddings[len(source_features):, 1],
               c='red', label='Target', alpha=0.5)

    plt.legend()
    plt.title('Feature Distribution Visualization')
    plt.savefig('results/domain_adaptation_visualization.png')
```

**Implementation Files**:
- `src/evaluation/domain_evaluation.py`
- `src/visualization/domain_visualization.py`

---

#### 3.5.2 **Model Selection Criteria**

**Primary Metrics** (Target Domain):
1. **Accuracy**: Overall classification accuracy
2. **F1-Score**: Weighted F1 across classes
3. **Worst-Class Performance**: Minimum per-class F1 (ensure no class neglect)

**Secondary Metrics**:
1. **Source Domain Performance**: Should not degrade significantly
2. **Domain Gap**: Source accuracy - Target accuracy (should be minimized)
3. **Calibration**: Expected Calibration Error (ECE) < 0.1
4. **Inference Speed**: Maintain real-time capability

**Selection Protocol**:
```python
def select_best_model(models, source_data, target_data):
    """Select best domain-adapted model"""
    results = []

    for model_name, model in models.items():
        # Evaluate
        target_acc = evaluate_accuracy(model, target_data)
        target_f1 = evaluate_f1(model, target_data)
        worst_class_f1 = min(evaluate_per_class_f1(model, target_data))
        source_acc = evaluate_accuracy(model, source_data)
        domain_gap = source_acc - target_acc

        # Composite score
        score = (
            0.4 * target_acc +
            0.3 * target_f1 +
            0.2 * worst_class_f1 +
            0.1 * (1 - domain_gap)  # Prefer smaller gap
        )

        results.append({
            'model': model_name,
            'score': score,
            'target_accuracy': target_acc,
            'target_f1': target_f1,
            'worst_class_f1': worst_class_f1,
            'domain_gap': domain_gap
        })

    # Sort by score
    results.sort(key=lambda x: x['score'], reverse=True)

    return results
```

---

#### 3.5.3 **Deployment Strategy**

**Deployment Pipeline**:

```
1. Initial Model Selection
   - Train all methods on source domain
   - Evaluate on held-out target domain validation set
   - Select top 3 performing methods

2. Ensemble Strategy (Optional)
   - Combine predictions from top 3 models
   - Weighted voting based on validation performance
   - Confidence-based selection (use most confident model per sample)

3. Active Learning Loop
   - Deploy model to new mine site
   - Identify low-confidence predictions
   - Request human labels for uncertain samples
   - Periodically retrain with new labels

4. Continuous Monitoring
   - Track prediction confidence distribution
   - Alert when confidence drops (domain shift detection)
   - Trigger retraining or adaptation
```

**Confidence-Based Routing**:

```python
class AdaptiveModelRouter:
    """Route samples to best model based on confidence"""

    def __init__(self, models):
        self.models = models  # Dictionary of models

    def predict(self, sample):
        # Get predictions from all models
        predictions = {}
        confidences = {}

        for name, model in self.models.items():
            logits = model(sample)
            probs = F.softmax(logits, dim=1)
            conf, pred = probs.max(dim=1)

            predictions[name] = pred
            confidences[name] = conf

        # Select most confident model
        best_model = max(confidences, key=confidences.get)

        return predictions[best_model], confidences[best_model], best_model
```

---

## 4. Implementation Roadmap

### 4.1 Development Timeline (13 Weeks)

| Week | Phase | Tasks | Deliverables |
|------|-------|-------|--------------|
| 1-2 | Foundation | Data infrastructure, baseline evaluation | Multi-domain dataset, baseline report |
| 3-4 | Core DA #1 | Adversarial domain adaptation | DANN implementation, training pipeline |
| 5-6 | Core DA #2 | MMD-based adaptation, contrastive learning | MMD implementation, results comparison |
| 7-8 | Advanced #1 | Self-training with pseudo-labels | Self-training pipeline, iteration results |
| 9 | Advanced #2 | Mining-specific augmentation, test-time adaptation | Augmentation module, TTA implementation |
| 10-11 | Multi-source | Multi-domain training, domain attention | Multi-source model, evaluation |
| 12 | Evaluation | Comprehensive evaluation, model selection | Performance report, visualizations |
| 13 | Deployment | Deployment pipeline, monitoring | Production-ready model, documentation |

### 4.2 Priority Ranking

**Tier 1 (Must Implement)**:
1. âœ… Adversarial Domain Adaptation (DANN)
2. âœ… MMD-based Adaptation
3. âœ… Self-Training with Pseudo-Labels
4. âœ… Mining-Specific Augmentation

**Tier 2 (High Value)**:
5. âš¡ Contrastive Domain Adaptation
6. âš¡ Test-Time Adaptation
7. âš¡ Multi-Source Domain Learning

**Tier 3 (Experimental)**:
8. ðŸ”¬ Meta-Learning for Few-Shot Adaptation
9. ðŸ”¬ Style Transfer for Domain Bridging
10. ðŸ”¬ Universal Domain Adaptation (for unknown classes)

---

## 5. Expected Outcomes

### 5.1 Performance Targets

**Baseline (Current)**:
- Source domain accuracy: 83-88%
- Target domain accuracy (zero-shot): ~40-60% (estimated)
- Domain gap: 25-45%

**Target After Domain Adaptation**:
- Source domain accuracy: â‰¥80% (maintain performance)
- Target domain accuracy: â‰¥75% (significant improvement)
- Domain gap: <10% (strong alignment)

**Per-Method Expected Improvements**:

| Method | Target Domain Accuracy | Domain Gap | Training Time | Inference Speed |
|--------|----------------------|------------|---------------|-----------------|
| Baseline (Zero-shot) | 50% | 35% | - | Fast |
| DANN | 72-75% | 10-15% | 1.5x | Fast |
| MMD | 70-73% | 12-17% | 1.3x | Fast |
| Self-Training | 75-78% | 8-12% | 2.0x | Fast |
| Contrastive DA | 73-76% | 10-14% | 1.4x | Fast |
| Test-Time Adapt | 68-71% | 15-20% | 1.0x | Slow (adaptive) |
| Multi-Source | 76-80% | 5-10% | 2.5x | Fast |
| **Ensemble** | **78-82%** | **5-8%** | - | Medium |

### 5.2 Success Criteria

âœ… **Primary Success**:
- Target domain accuracy > 75%
- All classes achieve F1 > 0.65
- Domain gap < 10%

âœ… **Secondary Success**:
- Source domain performance maintained (>80%)
- Inference time < 2x baseline
- Model size increase < 50%

âœ… **Stretch Goals**:
- Target domain accuracy > 80%
- Generalization to unseen 4th mine site > 70%
- Active learning reduces labeling effort by 80%

---

## 6. Resources & Requirements

### 6.1 Data Requirements

**Source Domain** (Training Site):
- Minimum: 1000 labeled images (current dataset)
- Optimal: 2000+ labeled images across all classes
- Balanced class distribution preferred

**Target Domain** (New Mine Sites):
- **Unsupervised DA**: 500-1000 unlabeled images per site
- **Few-Shot DA**: 10-50 labeled samples per class
- **Semi-Supervised DA**: 100-200 labeled + 500 unlabeled per site

**Multi-Source Learning**:
- 3-5 different source mine sites
- 500+ labeled images per site

### 6.2 Computational Requirements

**Training**:
- GPU: NVIDIA A100 (40GB) or V100 (32GB)
- RAM: 64GB+
- Storage: 500GB (datasets + checkpoints)
- Training time: 2-3 days per method

**Inference**:
- GPU: NVIDIA T4 or better
- RAM: 16GB
- Latency: <100ms per image (batch processing)

### 6.3 Software Dependencies

**New Libraries**:
```
# Domain adaptation
torch-domainadapt>=0.1.0

# Advanced augmentation
albumentations>=1.3.0

# Contrastive learning
pytorch-metric-learning>=1.7.0

# Visualization
umap-learn>=0.5.0
seaborn>=0.12.0

# Experiment tracking
wandb>=0.13.0
tensorboard>=2.11.0
```

---

## 7. Risk Mitigation

### 7.1 Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Insufficient target domain data | Medium | High | Use unsupervised DA methods (DANN, MMD) |
| Negative transfer (worse performance) | Low | High | Maintain source validation, early stopping |
| Overfitting to target domain | Medium | Medium | Regularization, hold-out target validation |
| Increased training time | High | Low | Start with efficient methods (MMD), parallelize |
| Label noise in pseudo-labels | High | Medium | Confidence thresholding, ensemble filtering |

### 7.2 Fallback Strategies

1. **If DA methods underperform**:
   - Fall back to strong augmentation + transfer learning
   - Use ensemble of source-trained models
   - Request more target domain labels

2. **If target domain data is limited**:
   - Prioritize few-shot meta-learning
   - Use style transfer for data augmentation
   - Active learning for efficient labeling

3. **If computational resources are limited**:
   - Start with MMD (no discriminator)
   - Use smaller batch sizes with gradient accumulation
   - Pre-extract features, train only top layers

---

## 8. Evaluation & Monitoring

### 8.1 Experiment Tracking

**Use Weights & Biases (wandb)**:

```python
import wandb

# Initialize experiment
wandb.init(
    project="mine-site-domain-adaptation",
    config={
        "method": "DANN",
        "lambda_domain": 0.5,
        "source_site": "Site_A",
        "target_site": "Site_B",
    }
)

# Log metrics
wandb.log({
    "train/class_loss": class_loss,
    "train/domain_loss": domain_loss,
    "val/source_accuracy": source_acc,
    "val/target_accuracy": target_acc,
    "val/domain_gap": domain_gap,
})

# Log model
wandb.save("models/best_dann_model.pth")
```

**Track Key Metrics**:
- Training loss curves (classification + domain)
- Source/Target accuracy over epochs
- Domain gap over training
- Per-class F1 scores
- Feature distribution visualizations (t-SNE)
- Confusion matrices

### 8.2 A/B Testing Protocol

**Deployment Testing**:
```
1. Deploy baseline model (A) and adapted model (B) in parallel
2. Route 10% of samples to each for comparison
3. Collect predictions + ground truth (human verification)
4. Monitor for 1 week
5. Statistical significance test (paired t-test)
6. Full rollout if B significantly outperforms A
```

---

## 9. Documentation & Knowledge Transfer

### 9.1 Documentation Deliverables

1. **Technical Report** (`docs/domain_adaptation_report.pdf`):
   - Methods overview
   - Implementation details
   - Results and analysis
   - Recommendations

2. **API Documentation** (`docs/api_reference.md`):
   - Usage examples
   - Configuration options
   - Model selection guide

3. **Training Guide** (`docs/training_guide.md`):
   - Step-by-step training instructions
   - Hyperparameter tuning tips
   - Troubleshooting common issues

4. **Deployment Guide** (`docs/deployment_guide.md`):
   - Model export and optimization
   - Integration instructions
   - Monitoring and maintenance

### 9.2 Code Organization

```
domain-adapt/
â”œâ”€â”€ classification_model/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ bma_mil_model.py (existing)
â”‚   â”‚   â”‚   â”œâ”€â”€ domain_adversarial_mil.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ mmd_domain_adaptive_mil.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ contrastive_domain_adaptive_mil.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ multi_domain_mil.py (NEW)
â”‚   â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”‚   â”œâ”€â”€ mmd_loss.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ contrastive_domain_loss.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ adversarial_loss.py (NEW)
â”‚   â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”‚   â”œâ”€â”€ domain_adversarial_training.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ mmd_training.py (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ self_training.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ multi_domain_training.py (NEW)
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”œâ”€â”€ multi_domain_dataset.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ multi_domain_loader.py (NEW)
â”‚   â”‚   â”œâ”€â”€ augmentation/
â”‚   â”‚   â”‚   â”œâ”€â”€ mining_augmentation.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ color_transformations.py (NEW)
â”‚   â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”‚   â”œâ”€â”€ domain_evaluation.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ cross_domain_metrics.py (NEW)
â”‚   â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_time_adaptation.py (NEW)
â”‚   â”‚   â”‚   â””â”€â”€ ensemble_inference.py (NEW)
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ gradient_reversal.py (NEW)
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ train_domain_adaptation.py (NEW)
â”‚   â”‚   â”œâ”€â”€ evaluate_cross_domain.py (NEW)
â”‚   â”‚   â””â”€â”€ compare_methods.py (NEW)
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ domain_adaptation_config.py (NEW)
â”‚   â””â”€â”€ notebooks/
â”‚       â”œâ”€â”€ domain_analysis.ipynb (NEW)
â”‚       â””â”€â”€ method_comparison.ipynb (NEW)
â””â”€â”€ docs/
    â”œâ”€â”€ domain_adaptation_report.pdf (NEW)
    â”œâ”€â”€ training_guide.md (NEW)
    â””â”€â”€ api_reference.md (NEW)
```

---

## 10. Future Directions

### 10.1 Advanced Research Directions

1. **Continual Learning**:
   - Learn from streaming data across multiple sites
   - Avoid catastrophic forgetting
   - Incremental class learning

2. **Open-Set Domain Adaptation**:
   - Detect novel spoil classes not in training data
   - "Unknown" class modeling
   - Out-of-distribution detection

3. **Explainable Domain Adaptation**:
   - Visualize what features transfer across domains
   - Identify domain-specific vs. domain-invariant features
   - Generate explanations for predictions

4. **Federated Learning**:
   - Train collaboratively across multiple mine sites
   - Preserve data privacy
   - Distributed domain adaptation

5. **Self-Supervised Pre-Training**:
   - Pre-train on unlabeled mining imagery
   - Learn mining-specific representations
   - Reduce reliance on ImageNet features

### 10.2 Practical Extensions

1. **Active Learning Integration**:
   - Identify most informative samples for labeling
   - Query strategy based on domain confusion
   - Budget-constrained labeling

2. **Multi-Modal Fusion**:
   - Incorporate metadata (GPS, time, camera model)
   - Geological survey data
   - Sensor data (moisture, composition)

3. **Temporal Adaptation**:
   - Handle seasonal changes at same mine site
   - Weather condition adaptation
   - Time-of-day invariance

4. **Mobile Deployment**:
   - Model compression for edge devices
   - Quantization and pruning
   - On-device adaptation

---

## 11. Conclusion & Recommendations

### 11.1 Recommended Approach

**Phase 1** (Immediate - Weeks 1-6):
1. âœ… Implement **Adversarial Domain Adaptation (DANN)**
   - Most mature and widely-used method
   - Strong theoretical foundation
   - Good balance of performance and complexity

2. âœ… Implement **MMD-based Adaptation**
   - Simpler than adversarial approach
   - No instability issues
   - Effective for covariate shift

3. âœ… Enhance **Data Augmentation**
   - Mining-specific color transformations
   - Illumination simulation
   - Fast wins with existing infrastructure

**Phase 2** (Medium-term - Weeks 7-11):
4. âš¡ **Self-Training Pipeline**
   - Leverage unlabeled target data
   - Progressive confidence thresholding
   - High impact for unlabeled scenarios

5. âš¡ **Multi-Source Learning**
   - If multiple mine sites available
   - Learn domain-agnostic features
   - Best long-term generalization

**Phase 3** (Advanced - Weeks 12-13):
6. ðŸ”¬ **Test-Time Adaptation**
   - Deploy-time flexibility
   - Continuous adaptation
   - Hedge against distribution shift

### 11.2 Key Success Factors

1. **Data Quality**: Ensure target domain data is representative
2. **Hyperparameter Tuning**: Careful tuning of Î»_domain, confidence thresholds
3. **Validation Strategy**: Hold-out target validation set is critical
4. **Iterative Development**: Start simple, add complexity gradually
5. **Monitoring**: Continuous performance tracking in production

### 11.3 Expected Impact

**Technical Impact**:
- **+20-30% accuracy improvement** on new mine sites
- **<10% domain gap** (from ~35%)
- **Reduced data labeling requirements** by 80%

**Business Impact**:
- Deploy to new mine sites with minimal retraining
- Faster rollout to new customers
- Reduced operational costs (less manual labeling)
- Improved safety and efficiency in mining operations

---

## References

### Key Papers

1. **Adversarial Domain Adaptation**:
   - Ganin et al. (2016). "Domain-Adversarial Training of Neural Networks". JMLR.
   - Tzeng et al. (2017). "Adversarial Discriminative Domain Adaptation". CVPR.

2. **MMD-Based Methods**:
   - Long et al. (2015). "Learning Transferable Features with Deep Adaptation Networks". ICML.
   - Long et al. (2017). "Conditional Adversarial Domain Adaptation". NeurIPS.

3. **Self-Training**:
   - Xie et al. (2020). "Self-training with Noisy Student improves ImageNet classification". CVPR.
   - Sohn et al. (2020). "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence". NeurIPS.

4. **Contrastive Learning**:
   - Khosla et al. (2020). "Supervised Contrastive Learning". NeurIPS.
   - Saito et al. (2021). "Cross-Domain Few-Shot Learning with Task-Specific Adapters". CVPR.

5. **Test-Time Adaptation**:
   - Wang et al. (2021). "Tent: Fully Test-Time Adaptation by Entropy Minimization". ICLR.
   - Sun et al. (2020). "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts". ICML.

6. **Multi-Source Domain Adaptation**:
   - Peng et al. (2019). "Moment Matching for Multi-Source Domain Adaptation". ICCV.
   - Zhao et al. (2020). "Multi-Source Distilling Domain Adaptation". AAAI.

### Domain Adaptation Surveys

- Wang & Deng (2018). "Deep Visual Domain Adaptation: A Survey". Neurocomputing.
- Wilson & Cook (2020). "A Survey of Unsupervised Deep Domain Adaptation". ACM TIST.
- Csurka (2021). "Domain Adaptation for Visual Applications: A Comprehensive Survey". Springer.

---

**Document Version**: 1.0
**Last Updated**: 2025-11-05
**Author**: Claude (AI Assistant)
**Status**: Ready for Implementation
